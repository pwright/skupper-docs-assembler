= Skupper
vlatest
:doctype: book
:page-component-name: skupper
:page-component-version: latest
:page-version: {page-component-version}
:page-component-display-version: latest
:page-component-title: Skupper

:docname: index
:page-module: ROOT
:page-relative-src-path: index.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#index:::]
== Preview homepage

This is a preview of link:https://github.com/skupperproject/skupper-docs[skupperproject/skupper-docs: Documentation for the Skupper project]
//external


This documentation is built using Antora and contains the following Antora modules:

* https://github.com/skupperproject/skupper-docs/tree/main/modules/ROOT[ROOT] - Metadata like this page and images
* https://github.com/skupperproject/skupper-docs/tree/main/modules/cli[cli] - Kubernetes and Podman CLI procedures
* https://github.com/skupperproject/skupper-docs/tree/main/modules/cli-reference[cli-reference] - Kubernetes CLI reference (generated)
* https://github.com/skupperproject/skupper-docs/tree/main/modules/console[console] - Console docs
* https://github.com/skupperproject/skupper-docs/tree/main/modules/declarative[declarative] - Using Skupper with YAML
* https://github.com/skupperproject/skupper-docs/tree/main/modules/overview[overview] - Skupper concepts
* https://github.com/skupperproject/skupper-docs/tree/main/modules/operator[operator] - Using Skupper with a Kubernetes operator
* https://github.com/skupperproject/skupper-docs/tree/main/modules/policy[policy] - Control Skupper usage in a cluster using the policy CRD
* https://github.com/skupperproject/skupper-docs/tree/main/modules/podman-reference[podman-reference] - Podman CLI reference (generated)
* https://github.com/skupperproject/skupper-docs/tree/main/modules/troubleshooting[troubleshooting] - Resolving problems in your service network

NOTE: This ROOT module is never published but contains partials that are used in various documents.

:!sectids:
== <strong>Basics</strong>
:sectids:

:docname: index
:page-module: overview
:page-relative-src-path: index.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#overview:index:::]
== Overview
//Category: skupper-overview
// Type: assembly
[id="skupper-overview"]

Hybrid clouds enable organizations to combine on-premises, private cloud, and public cloud resources.
While such a solution provides many benefits, it also presents a unique challenge: enabling these resources to communicate with each other.

Skupper provides a solution to this challenge with a Virtual Application Network that simply and securely connects applications running in different network locations.

// Type: concept
[discrete#overview:index:::virtual-application-networks,id="virtual-application-networks"]
=== Virtual Application Networks

Skupper solves multi-cluster communication challenges through something called a Virtual Application Network (VAN).
To understand the value of Skupper, it is helpful to first understand what a VAN is.

A VAN connects the applications and services in your hybrid cloud into a virtual network so that they can communicate with each other as if they were all running in the same site.
In this diagram, a VAN connects three services, each of which is running in a different cloud:

image::skupper/latest/overview/_images/overview-clouds.png[]

In essence, the VAN connects the services in a distributed application with a microservice architecture.

image::skupper/latest/overview/_images/overview-application.png[]

VANs are able to provide connectivity across the hybrid cloud because they operate at Layer 7 (the application layer).
They use *Layer 7 application routers* to route communication between *Layer 7 application addresses*.

// Type: concept
[discrete#overview:index:::layer-7-application-routers,id="layer-7-application-routers"]
=== Layer 7 application routers

Layer 7 application routers form the backbone of a VAN in the same way that conventional network routers form the backbone of a VPN.
However, instead of routing IP packets between network endpoints, Layer 7 application routers route messages between application endpoints (called Layer 7 application addresses).

// Type: concept
[discrete#overview:index:::layer-7-application-addresses,id="layer-7-application-addresses"]
=== Layer 7 application addresses

A Layer 7 application address represents an endpoint, or destination in the VAN.
When an application sends a communication to an address, the Layer 7 application routers distribute the communication to any other application in the VAN that has the same address.

For example, in this diagram, *Service B* sends a message with an application address to its local application router.
*Service A* and *Service C* are subscribed to the same address, so the application router routes copies of the message through the VAN until they arrive at each destination.

image::skupper/latest/overview/_images/overview-routers.png[]

VANs provide multiple routing patterns, so communications can be distributed in anycast (balanced or closest) or multicast patterns.

// Type: concept
[discrete#overview:index:::skupper,id="skupper"]
=== Skupper

Skupper is an open source tool for creating VANs in Kubernetes.
By using Skupper, you can create a distributed application consisting of microservices running in different Kubernetes clusters.

This diagram illustrates a Skupper network that connects three services running in three different Kubernetes clusters:

image::skupper/latest/overview/_images/overview-clusters.png[]

In a Skupper network, each namespace contains a Skupper instance.
When these Skupper instances connect, they continually share information about the services that each instance exposes.
This means that each Skupper instance is always aware of every service that has been exposed to the Skupper network, regardless of the namespace in which each service resides.

Once a Skupper network is formed across Kubernetes namespaces, any of the services in those namespaces can be exposed (through annotation) to the Skupper network.
When a service is exposed, Skupper creates proxy endpoints to make that service available on each namespace in the Skupper network.

.Additional information

* <<overview:security:::>>
* <<overview:routing:::>>
* <<overview:connectivity:::>>

:docname: security
:page-module: overview
:page-relative-src-path: security.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#overview:security:::]
== Security
//Category: skupper-security
// Type: assembly
[id="skupper-security"]

Skupper securely connects your services with TLS authentication and encryption.
See how Skupper enables you to deploy your application securely across Kubernetes clusters.

// Type: concept
[discrete#overview:security:::skupper-security-challenges,id="skupper-security-challenges"]
=== Security challenges in the cloud

Moving an application to the cloud raises security risks.
Either your services must be exposed to the public internet, or you must adopt complex layer 3 network controls like VPNs, firewall rules, and access policies.

Increasing the challenge, layer 3 network controls do not extend easily to multiple clusters.
These network controls must be duplicated for each cluster.

// Type: concept
[discrete#overview:security:::service-network-isolation,id="service-network-isolation"]
=== Built-in network isolation

Skupper provides default, built-in security that scales across clusters and clouds.
In a Skupper network, the connections between Skupper routers are secured with mutual TLS using a private, dedicated certificate authority (CA).
Each router is uniquely identified by its own certificate.

image::skupper/latest/overview/_images/clusters-tls.svg[]

This means that the Skupper network is isolated from external access, preventing security risks such as lateral attacks, malware infestations, and data exfiltration.

.Additional information

.Additional information

* <<overview:index:::>>
* <<overview:connectivity:::>>
* <<overview:routing:::>>

:docname: connectivity
:page-module: overview
:page-relative-src-path: connectivity.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#overview:connectivity:::]
== Connectivity

Skupper represents a new approach to connecting services across multiple Kubernetes clusters.
See how Skupper can give you the flexibility to deploy your services where you need them.

[discrete#overview:connectivity:::one-cluster]
=== One cluster

Kubernetes *services* provide a virtual network address for each element of your distributed application.
Service "A" can contact service "B", "B" can contact "C", and so on.

image::skupper/latest/overview/_images/one-cluster.svg[]

But if you want to deploy your application across multiple clusters, your options are limited.
You have to either expose your services to the public internet or set up a VPN.

Skupper offers a third way.
It connects clusters to a secure layer 7 network.
It uses that network to forward local service traffic to remote clusters.

[discrete#overview:connectivity:::secure-hybrid-cloud-communication]
=== Secure hybrid cloud communication

Deploy your application across public and private clusters.

image::skupper/latest/overview/_images/two-clusters.svg[]

You can host your database on a private cluster and retain full connectivity with services running on the public cloud.
All communication is secured by mutual TLS authentication and encryption.

[discrete#overview:connectivity:::edge-to-edge-connectivity]
=== Edge-to-edge connectivity

Distribute application services across geographic regions.

image::skupper/latest/overview/_images/five-clusters.svg[]

You can connect multiple retail sites to a central office.
Once connected, each edge location can contact any other edge.
You can add and remove sites on demand.

[discrete#overview:connectivity:::scale-up-and-out]
=== Scale up and out

Build large, robust networks of connected clusters.

image::skupper/latest/overview/_images/many-clusters.svg[]

.Additional information

* <<overview:index:::>>
* <<overview:routing:::>>
* <<overview:connectivity:::>>

:docname: routing
:page-module: overview
:page-relative-src-path: routing.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#overview:routing:::]
== Routing
//Category: skupper-routing
// Type: assembly
[id="skupper-routing"]

Skupper uses layer 7 addressing and routing to connect services.
See how the power of application-layer addressing can bring new capabilities to your applications.

// Type: concept
[discrete#overview:routing:::multi-cluster-services,id="multi-cluster-services"]
=== Multi-cluster services

Deploy a single logical service across multiple clusters.

Skupper can route requests to instances of a single service running on multiple clusters.
If a provider or data center fails, the service instances running at unaffected sites can scale to meet the need and maintain availability.

// Type: concept
[discrete#overview:routing:::dynamic-load-balancing,id="dynamic-load-balancing"]
=== Dynamic load balancing

Balance requests across clusters according to service capacity.

The Skupper network has cross-cluster visibility.
It can see which services are already loaded and which have spare capacity, and it directs requests accordingly.

You can assign a cost to each inter-cluster connection.
This enables you to configure a preference for one resource over another.
If demand is normal, you can keep all traffic on your private cloud.
If demand peaks, you can dynamically spill over to public cloud resources.

// Type: concept
[discrete#overview:routing:::reliable-networks,id="reliable-networks"]
=== Reliable networks

Skupper uses redundant network paths and smart routing to provide highly available connectivity at scale.

.Additional information

* <<overview:index:::>>
* <<overview:security:::>>
* <<overview:connectivity:::>>

:!sectids:
== <strong>Kubernetes</strong>
:sectids:

:!sectids:
== https://skupper.io/start/index.html[Getting started]
:sectids:

:docname: index
:page-module: cli
:page-relative-src-path: index.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#cli:index:::]
== CLI guide
//Category: skupper-cli
// Type: assembly
[id="skupper-cli"]

Using the `skupper` command-line interface (CLI) allows you to create and manage Skupper sites from the context of the current namespace.

A typical workflow is to create a site, link sites together, and expose services to the {service-network}.

// Type: procedure
[discrete#cli:index:::installing-cli,id="installing-cli"]
=== Installing the Skupper CLI


Installing the `skupper` command-line interface (CLI) provides a simple method to get started with Skupper.

.Procedure

. Install the `skupper` command-line interface.
+
--
For Linux:
[subs=attributes+]
----
$ curl -fL https://github.com/skupperproject/skupper/releases/download/{skupper-version}/skupper-cli-{skupper-version}-linux-amd64.tgz | tar -xzf -
----

For MacOS:
[subs=attributes+]
----
$ curl -fL https://github.com/skupperproject/skupper/releases/download/{skupper-version}/skupper-cli-{skupper-version}-mac-amd64.tgz | tar -xzf -
----
--

. Copy the `skupper` executable to a directory in your $PATH:
+
----
$ mkdir -p $HOME/bin
$ export PATH=$PATH:$HOME/bin
$ mv skupper $HOME/bin
----


. Verify the installation.
+
[subs=attributes+]
----
$ skupper version
client version {skupper-cli-version}
----

// Type: procedure
[discrete#cli:index:::creating-using-cli,id="creating-using-cli"]
=== Creating a site using the CLI

A {service-network} consists of Skupper sites.
This section describes how to create a site using the default settings.

.Prerequisites

* The `skupper` CLI is installed.
* You are logged into the cluster.
* The services you want to expose on the {service-network} are in the active namespace.


.Procedure

. Create a default site:
+
----
$ skupper init
----
+
Starting with Skupper release 1.3, the console is not enabled by default.
To use the new console, which is a preview feature and may change, see <<console:index:::>>.

. Check the site:
+
----
$ skupper status

Skupper is enabled for namespace "west" in interior mode. It is not connected to any other sites.
----
+
NOTE: The default message above is displayed when you initialize a site on a cluster that does not have the policy system installed.
If you install the policy system as described in <<policy:index:::>>, the message becomes `Skupper is enabled for namespace "west" in interior mode (with policies)`.

By default, the site name defaults to the namespace name, for example, `west`.


// Type: reference
[discrete#cli:index:::custom-sites,id="custom-sites"]
=== Custom sites

The default `skupper init` creates sites that satisfy typical requirements.

Starting with Skupper release 1.3, the console is not enabled by default.
To use the new console, which is a preview feature and may change, see <<console:index:::>>.

If you require a custom configuration, note the following options:

* Configuring console authentication.
There are several `skupper` options regarding authentication for the console:
+
--

`--console-auth <authentication-mode>`:: Set the authentication mode for the console:

* `openshift` - Use OpenShift authentication, so that users who have permission to log into OpenShift and view the Project (namespace) can view the console.
* `internal` -  Use Skupper authentication, see the `console-user` and `console-password` options.
* `unsecured` - No authentication, anyone with the URL can view the console.


`--console-user <username>`:: Username for the console user when authentication mode is set to `internal`.
Defaults to `admin`.
`--console-password <password>`:: Password for the console user when authentication mode is set to `internal`.
If not specified, a random passwords is generated.
--



* Configuring service access
+
--

----
$ skupper init --create-network-policy
----

NOTE: All sites are associated with a namespace, called the _active namespace_ in this procedure.

Services in the active namespace may be accessible to pods in other namespaces on that cluster by default, depending on your cluster network policies.
As a result, you can expose services to pods in namespaces not directly connected to the {service-network}.
This setting applies a network policy to restrict access to services to those pods in the active namespace.

For example, if you create a site in the namespace `projectA` of `clusterA` and link that site to a {service-network} where the `database` service is exposed, the `database` service is available to pods in `projectB` of `clusterA`.

You can use the `--create-network-policy` option to restrict the `database` service access to `projectA` of `clusterA`.

--

// Type: procedure
[discrete#cli:index:::linking-sites,id="linking-sites"]
=== Linking sites

A {service-network} consists of Skupper sites.
This section describes how to link sites to form a {service-network}.

Linking two sites requires a single initial directional connection. However:

* Communication between the two sites is bidirectional, only the initial linking is directional.
* The choice of direction for linking is typically determined by accessibility. For example, if you are linking an OpenShift Dedicated cluster with a CodeReady Containers cluster, you must link from the CodeReady Containers cluster to the OpenShift Dedicated cluster because that route is accessible.

.Procedure

. Determine the direction of the link. If both clusters are publicly addressable, then the direction is not significant. If one of the clusters is addressable from the other cluster, perform step 2 below on the addressable cluster.

. Generate a token on the cluster that you want to link to:
+
--
[source, bash]
----
$ skupper token create <filename>
----

where `<filename>` is the name of a YAML file that is saved on your local filesystem.

This file contains a key and the location of the site that created it.

[NOTE]
====
Access to this file provides access to the {service-network}.
Protect it appropriately.

For more information about protecting access to the {service-network}, see <<cli:tokens:::>>.
====
--

. Use a token on the cluster that you want to connect from:

.. Create a link to the {service-network}:
+
----
$ skupper link create <filename> [-name <link-name>]
----
+
where `<filename>` is the name of a YAML file generated from the `skupper token create` command and `<link-name>` is the name of the link.

.. Check the link:
+
----
$ skupper link status
Link link1 not connected
----
+
In this example no <link-name> was specified, the name defaulted to `link1`.


. If you want to delete a link:
+
----
$ skupper link delete <link-name>
----
where `<link-name>` is the name of the link specified during creation.

// Type: procedure
[discrete#cli:index:::link-cost,id="link-cost"]
=== Specifying link cost

When linking sites, you can assign a cost to each link to influence the traffic flow.
By default, link cost is set to `1` for a new link.
In a {service-network}, the routing algorithm attempts to use the path with the lowest total cost from client to target server.

* If you have services distributed across different sites, you might want a client to favor a particular target or link.
In this case, you can specify a cost of greater than `1` on the alternative links to reduce the usage of those links.
+
NOTE: The distribution of open connections is statistical, that is, not a round robin system.

* If a connection only traverses one link, then the path cost is equal to the link cost.
If the connection traverses more than one link, the path cost is the sum of all the links involved in the path.

* Cost acts as a threshold for using a path from client to server in the network.
When there is only one path, traffic flows on that path regardless of cost.
+
NOTE: If you start with two targets for a service, and one of the targets is no longer available, traffic flows on the remaining path regardless of cost.

* When there are a number of paths from a client to server instances or a service, traffic flows on the lowest cost path until the number of connections exceeds the cost of an alternative path.
After this threshold of open connections is reached, new connections are spread across the alternative path and the lowest cost path.


.Prerequisite

* You have set your Kubernetes context to a site that you want to link _from_.
* A token for the site that you want to link _to_.

.Procedure

. Create a link to the {service-network}:
+
--
[source, bash]
----
$ skupper link create <filename> --cost <integer-cost>
----

where `<integer-cost>` is an integer greater than 1 and traffic favors lower cost links.

NOTE: If a service can be called without traversing a link, that service is considered local, with an implicit cost of `0`.

For example, create a link with cost set to `2` using a token file named `token.yaml`:

----
$ skupper link create token.yaml --cost 2
----
--

. Check the link cost:
+
----
$ skupper link status link1 --verbose

 Cost:          2
 Created:       2022-11-17 15:02:01 +0000 GMT
 Name:          link1
 Namespace:     default
 Site:          default-0d99d031-cee2-4cc6-a761-697fe0f76275
 Status:        Connected
----

. Observe traffic using the console.
+
--
If you have a console on a site, log in and navigate to the processes for each server.
You can view the traffic levels corresponding to each client.

NOTE: If there are multiple clients on different sites, filter the view to each client to determine the effect of cost on traffic.
For example, in a two site network linked with a high cost with servers and clients on both sites, you can see that a client is served by the local servers while a local server is available.
--

// Type: concept
[discrete#cli:index:::exposing-services-ns,id="exposing-services-ns"]
=== Exposing services on the {service-network} from a namespace

After creating a {service-network}, exposed services can communicate across that network.

The `skupper` CLI has two options for exposing services that already exist in a namespace:

* `expose` supports simple use cases, for example, a deployment with a single service.
See <<cli:index:::exposing-simple-services>> for instructions.

* `service create` and `service bind` is a more flexible method of exposing services, for example, if you have multiple services for a deployment.
See <<cli:index:::exposing-complex-services>> for instructions.


// Type: procedure
[discrete#cli:index:::exposing-simple-services,id="exposing-simple-services"]
==== Exposing simple services on the {service-network}
This section describes how services can be enabled for a {service-network} for simple use cases.

.Procedure

. Create a deployment, some pods, or a service in one of your sites, for example:
+
----
$ kubectl create deployment hello-world-backend --image quay.io/skupper/hello-world-backend
----
+
This step is not Skupper-specific, that is, this process is unchanged from standard processes for your cluster.

. Create a service that can communicate on the {service-network}:
+
--
[source, bash]
----
$ skupper expose [deployment <name>|pods <selector>|statefulset <statefulsetname>|service <name>]
----

where

* `<name>` is the name of your deployment
* `<selector>` is a pod selector
* `<statefulsetname>` is the name of a statefulset

For the example deployment in step 1, you create a service using the following command:
----
$ skupper expose deployment/hello-world-backend --port 8080
----

Options for this command include:

* `--port <port-number>`:: Specify the port number that this service is available on the {service-network}.
NOTE: You can specify more than one port by repeating this option.

* `--target-port <port-number>`:: Specify the port number of pods that you want to expose.

* `--protocol <protocol>` allows you specify the protocol you want to use, `tcp`, `http` or `http2`

--

NOTE: If you do not specify ports, `skupper` uses the `containerPort` value of the deployment.

// Type: procedure
[discrete#cli:index:::exposing-complex-services,id="exposing-complex-services"]
==== Exposing complex services on the {service-network}

This section describes how services can be enabled for a {service-network} for more complex use cases.

.Procedure

. Create a deployment, some pods, or a service in one of your sites, for example:
+
----
$ kubectl create deployment hello-world-backend --image quay.io/skupper/hello-world-backend
----
+
This step is not Skupper-specific, that is, this process is unchanged from standard processes for your cluster.

. Create a service that can communicate on the {service-network}:
+
--
[source, bash]
----
$ skupper service create <name> <port>
----

where

* `<name>` is the name of the service you want to create
* `<port>` is the port the service uses

For the example deployment in step 1, you create a service using the following command:
----
$ skupper service create hello-world-backend 8080
----


--

. Bind the service to a cluster service:
+
--
[source, bash]
----
$ skupper service bind <service-name> <target-type> <target-name>
----

where

* `<service-name>` is the name of the service on the {service-network}

* `<target-type>` is the object you want to expose, `deployment`, `statefulset`, `pods`, or `service`.

* `<target-name>` is the name of the cluster service

For the example deployment in step 1, you bind the service using the following command:
----
$ skupper service bind hello-world-backend deployment hello-world-backend
----

--

// Type: procedure
[discrete#cli:index:::exposing-services-from-different-ns,id="exposing-services-from-different-ns"]
==== Exposing services from a different namespace to the {service-network}

This section shows how to expose a service from a namespace where Skupper is not deployed.

Skupper allows you expose Kubernetes services from other namespaces for any site.
However, if you want to expose workloads, for example deployments, you must create a site as described in this section.

.Prerequisites

* A namespace where Skupper is deployed.
* A network policy that allows communication between the namespaces
* cluster-admin permissions if you want to expose resources other than services


. Create a site with cluster permissions if you want to expose a workload from a namespace other than the site namespace:
+
--
[source, bash]
----
$ skupper init --enable-cluster-permissions
----
--


. Expose the service on the {service-network}:
+
NOTE: The site does not require the extra permissions granted with the `--enable-cluster-permissions` to expose a Kubernetes service.

.. If you want to expose a Kubernetes service from a namespace other than the site namespace:
+
--
[source, bash]
----
$ skupper expose service <service>.<namespace> --address <service>
----

* <service> - the name of the service on the {service-network}.
* <namespace> - the name of the namespace where the service you want to expose runs.

For example, if you deployed Skupper in the `east` namespace and you created a `backend` Kubernetes service in the `east-backend` namespace, you set the context to the `east` namespace and expose the service as `backend` on the {service-network} using:

----
$ skupper expose service backend.east-backend --port 8080 --address backend
----
--

.. If you want to expose a workload and you created a site with `--enable-cluster-permissions`:
+
--
[source, bash]
----
$ skupper expose <resource> --port <port-number> --target-namespace <namespace>
----

* <resource> - the name of the resource.
* <namespace> - the name of the namespace where the resource you want to expose runs.

For example, if you deployed Skupper in the `east` namespace and you created a `backend` deployment in the `east-backend` namespace, you set the context to the `east` namespace and expose the service as `backend` on the {service-network} using:

----
$ skupper expose deployment/backend --port 8080 --target-namespace east-backend
----
--


// Type: concept
[discrete#cli:index:::exposing-services-local,id="exposing-services-local"]
=== Exposing services on the {service-network} from a local machine

After creating a {service-network}, you can expose services from a local machine on the {service-network}.

For example, if you run a database on a server in your data center, you can deploy a front end in a cluster that can access the data as if the database was running in the cluster.

// Type: procedure
[discrete#cli:index:::exposing-service-gateway,id="exposing-service-gateway"]
==== Exposing simple local services to the {service-network}

This section shows how to expose a single service running locally on a {service-network}.

.Prerequisites

* A {service-network}. Only one site is required.
* Access to the {service-network}.


.Procedure

. Run your service locally.

. Log into your cluster and change to the namespace for your site.

. Expose the service on the {service-network}:
+
--
[source, bash]
----
$ skupper gateway expose <service> localhost <port>
----

* <service> - the name of the service on the {service-network}.
* <port> - the port that runs the service locally.

[NOTE]
====
You can also expose services from other machines on your local network, for example if MySQL is running on a dedicated server (with an IP address of `192.168.1.200`), but you are accessing the cluster from a machine in the same network:

----
$ skupper gateway expose mysql 192.168.1.200 3306
----
====
--

. Check the status of Skupper gateways:
+
--

[subs=attributes+]
----
$ skupper gateway status

Gateway Definition:
╰─ machine-user type:service version:{service-version}
   ╰─ Bindings:
      ╰─ mydb:3306 tcp mydb:3306 localhost 3306

----
This shows that there is only one exposed service and that service is only exposing a single port (BIND). There are no ports forwarded to the local host.

The URL field shows the underlying communication and can be ignored.
--

// Type: procedure
[discrete#cli:index:::exposing-services-gateway,id="exposing-services-gateway"]
==== Working with complex local services on the {service-network}


This section shows more advanced usage of skupper gateway.

. Create a Skupper gateway:
+
--
[source,bash]
----
$ skupper gateway init --type <gateway-type>
----



By default a _service_ type gateway is created, however you can also specify:

* `podman`
* `docker`
--

. Create a service that can communicate on the {service-network}:
+
--
[source, bash]
----
$ skupper service create <name> <port>
----

where

* `<name>` is the name of the service you want to create
* `<port>` is the port the service uses

For example:

[source, bash]
----
$ skupper service create mydb 3306
----
--

. Bind the service on the {service-network}:
+
--
[source, bash]
----
$ skupper gateway bind <service> <host> <port>
----

* <service> - the name of the service on the {service-network}, `mydb` in the example above.
* <host> - the host that runs the service.
* <port> - the port the service is running on, `3306` from the example above.
--

. Check the status of Skupper gateways:
+
--
[source, bash, subs=attributes+]
----
$ skupper gateway status
Gateway Definitions Summary

Gateway Definition:
╰─ machine-user type:service version:{service-version}
   ╰─ Bindings:
      ╰─ mydb:3306 tcp mydb:3306 localhost 3306

----
This shows that there is only one exposed service and that service is only exposing a single port (BIND). There are no ports forwarded to the local host.

The URL field shows the underlying communication and can be ignored.

You can create more services in the {service-network} and bind more local services to expose those services on the {service-network}.
--

. Forward a service from the {service-network} to the local machine.
+
--
[source, bash]
----
$ skupper gateway forward <service> <port>
----

where

* `<service>` is the name of an existing service on the {service-network}.
* `<port>` is the port on the local machine that you want to use.

--


// Type: procedure
[discrete#cli:index:::exporting-gateway,id="exporting-gateway"]
==== Creating a gateway and applying it on a different machine

If you have access to a cluster from one machine but want to create a gateway to the {service-network} from a different machine, you can create the gateway definition bundle on the first machine and later apply that definition bundle on a second machine as described in this procedure.
For example, if you want to expose a local database service to the {service-network}, but you never want to access the cluster from the database server, you can use this procedure to create the definition bundle and apply it on the database server.

.Procedure

. Log into your cluster from the first machine and change to the namespace for your site.


. Create a service that can communicate on the {service-network}:
+
--
[source, bash]
----
$ skupper service create <name> <port>
----

where

* `<name>` is the name of the service you want to create
* `<port>` is the port the service uses

For example:

[source, bash]
----
$ skupper service create database 5432
----
--

. Create a YAML file to represent the service you want to expose, for example:
+
--
[source,yaml]
----
name: database <1>
bindings:
    - name: database <2>
      host: localhost <3>
      service:
        address: database:5432 <4>
        protocol: tcp <5>
        ports:
            - 5432 <6>
      target_ports:
        - 5432 <7>
qdr-listeners:
    - name: amqp
      host: localhost
      port: 5672
----
<1> Gateway name, useful for reference only.
<2> Binding name, useful to track multiple bindings.
<3> Name of host providing the service you want to expose.
<4> Service name and port on {service-network}. You created the service in a previous step.
<5> The protocol you want to use to expose the service, `tcp`, `http` or `http2`.
<6> The port on the {service-network} that you want this service to be available on.
<7> The port of the service running on the host specified in point 3.

--

. Save the YAML file using the name of the gateway, for example, `gateway.yaml`.

. Generate a bundle that can be applied to the machine that hosts the service you want to expose on the {service-network}:
+
--
[source, bash]
----
$ skupper gateway generate-bundle <config-filename> <destination-directory>
----

where:

* <config-filename> - the name of the YAML file, including suffix, that you generated in the previous step.
* <destination-directory> - the location where you want to save the resulting gateway bundle, for example `~/gateways`.

For example:
[source, bash]
----
$ skupper gateway generate-bundle database.yaml ./
----

This bundle contains the gateway definition YAML and a  certificate that allow access to the {service-network}.

--

. Copy the gateway definition file, for example, `mylaptop-jdoe.tar.gz` to the machine that hosts the service you want to expose on the {service-network}.

. From the machine that hosts the service you want to expose:
+
--
[source, bash]
----
$ mkdir gateway

$ tar -xvf <gateway-definition-file> --directory gateway
$ cd gateway
$ sh ./launch.py
----

NOTE: Use `./launch.py -t podman` or `./launch.py -t docker` to run the Skupper router in a container.

Running the gateway bundle uses the gateway definition YAML and a certificate to access and expose the service on the {service-network}.

--

. Check the status of the gateway service:
+
--

To check a _service_ type gateway:
[source, bash]
----
$ systemctl --user status <gateway-definition-name>
----

To check a _podman_ type gateway:
[source, bash]
----
$ podman inspect
----

To check a _docker_ type gateway:
[source, bash]
----
$ docker inspect
----


NOTE: You can later remove the gateway using `./remove.py`.

--

. From the machine with cluster access, check the status of Skupper gateways:
+
--
[subs=attributes+]
----
$ skupper gateway status
Gateway Definition:
╰─ machine-user type:service version:{service-version}
   ╰─ Bindings:
      ╰─ mydb:3306 tcp mydb:3306 localhost 3306
----
This shows that there is only one exposed service and that service is only exposing a single port (BIND). There are no ports forwarded to the local host.
--

NOTE: If you need to change the gateway definition, for example to change port, you need to remove the existing gateway and repeat this procedure from the start to redefine the gateway.

// Type: procedure
[discrete#cli:index:::gateway-reference,id="gateway-reference"]
==== Gateway YAML reference

The <<cli:index:::exporting-gateway>> describes how to create a gateway to apply on a separate machine using a gateway definition YAML file.

The following are valid entries in a gateway definition YAML file.

name:: Name of gateway
bindings.name:: Name of binding for a single host.
bindings.host:: Hostname of local service.
bindings.service:: Definition of service you want to be available on service network.
bindings.service.address:: Address on the service network, name and port.
bindings.service.protocol:: Skupper protocol, `tcp`, `http` or `http2`.
bindings.service.ports:: A single port that becomes available on the service network.
bindings.service.target_ports:: A single port that you want to expose on the service network.

NOTE: If the local service requires more than one port, create separate bindings for each port.

forwards.name:: Name of forward for a single host.
forwards.host:: Hostname of local service.
forwards.service:: Definition of service you want to be available locally.
forwards.service.address:: Address on the service network that you want to use locally, name and port.
forwards.service.protocol:: Skupper protocol, `tcp`, `http` or `http2`.
forwards.service.ports:: A single port that is available on the service network.
forwards.service.target_ports:: A single port that you want to use locally.

NOTE: If the network service requires more than one port, create separate forwards for each port.

qdr-listeners:: Definition of skupper router listeners
qdr-listeners.name:: Name of skupper router, typically `amqp`.
qdr-listeners.host:: Hostname for skupper router, typically `localhost`.
qdr-listeners.port:: Port for skupper router, typically `5672`.



// Type: procedure
[discrete#cli:index:::network-service,id='network-service']
=== Exploring a {service-network}

Skupper includes a command to allow you report all the sites and the services available on a {service-network}.

.Prerequisites

* A {service-network} with more than one site

.Procedure

. Set your Kubernetes context to a namespace on the {service-network}.

. Use the following command to report the status of the {service-network}:
+
--

[source,bash]
----
$ skupper network status
----

For example:

[source]
----
Sites:
├─ [local] 4dba248 - west  <1>
│  URL: 10.96.146.236 <2>
│  name: west <3>
│  namespace: west
│  version: 0.8.6 <4>
│  ╰─ Services:
│     ╰─ name: hello-world-backend <5>
│        address: hello-world-backend: 8080 <6>
│        protocol: tcp <7>
╰─ [remote] bca99d1 - east <8>
   URL:
   name: east
   namespace: east
   sites linked to: 4dba248-west <9>
   version: 0.8.6
   ╰─ Services:
      ╰─ name: hello-world-backend
         address: hello-world-backend: 8080
         protocol: tcp
         ╰─ Targets:
            ╰─ name: hello-world-backend-7dfb45b98d-mhskw <10>
----

<1> The unique identifier of the site associated with the current context, that is, the `west` namespace

<2> The URL of the {service-network} router.
This is required for other sites to connect to this site and is different from the console URL.
If you require the URL of the console, use the `skupper status` command to display that URL.

<3> The site name.
By default, skupper uses the name of the current namespace.
If you want to specify a site name, use `skupper init  --site-name <site-name>`.

<4> The version of Skupper running the site.
The site version can be different from the current `skupper` CLI version.
To update a site to the version of the CLI, use `skupper update`.

<5> The name of a service exposed on the {service-network}.

<6> The address of a service exposed on the {service-network}.

<7> The protocol of a service exposed on the {service-network}.

<8> The unique identifier of a remote site on the {service-network}.

<9> The sites that the remote site is linked to.

<10> The name of the local Kubernetes object that is exposed on the {service-network}.
In this example, this is the `hello-world-backend` pod.

[NOTE]
====
The URL for the east site has no value because that site was initialized without ingress using the following command:
----
$ skupper init --ingress none
----
====
--

// Type: assembly
[discrete#cli:index:::built-in-security-options,id="built-in-security-options"]
=== Securing a {service-network}

Skupper provides default, built-in security that scales across clusters and clouds.
This section describes additional security you can configure.

See <<policy:index:::>> for information about creating granular policies for each cluster.

// Type: procedure
[discrete#cli:index:::network-policy,id="network-policy"]
==== Restricting access to services using network-policy

By default, if you expose a service on the {service-network}, that service is also accessible from other namespaces in the cluster.
You can avoid this situation when creating a site using the `--create-network-policy` option.

.Procedure

. Create the {service-network} router with a network policy:
+
[source,bash]
----
$ skupper init --create-network-policy
----

. Check the site status:
+
--
[source,bash]
----
$ skupper status
----
The output should be similar to the following:
----
Skupper enabled for namespace 'west'. It is not connected to any other sites.
----
--

You can now expose services on the {service-network} and those services are not accessible from other namespaces in the cluster.


// Type: procedure
[discrete#cli:index:::tls,id="tls"]
==== Applying TLS to TCP or HTTP2 traffic on the {service-network}

By default, the traffic between sites is encrypted, however the traffic between the service pod and the router pod is not encrypted.
For services exposed as TCP or HTTP2, the traffic between the pod and the router pod can be encrypted using TLS.

.Prerequisites

* Two or more linked sites
* A TCP or HTTP2 frontend and backend service

.Procedure

. Deploy your backend service.

. Expose your backend deployment on the {service-network}, enabling TLS.
+
For example, if you want to expose a TCP service:
--
[source,bash]
----
$ skupper expose deployment <deployment-name> --port 443 --enable-tls
----

Enabling TLS creates the necessary certificates required for TLS backends and stores them in a secret named `skupper-tls-<deployment-name>`.
--

. Modify the backend deployment to include the generated certificates, for example:
+
--
[source,yaml]
----
...
    spec:
      containers:
      ...
        command:
        ...
        - "/certs/tls.key"
        - "/certs/tls.crt"
        ...
        volumeMounts:
        ...
        - mountPath: /certs
          name: certs
          readOnly: true
      volumes:
      - name: index-html
        configMap:
          name: index-html
      - name: certs
        secret:
          secretName: skupper-tls-<deployment-name>
----

Each site creates the necessary certificates required for TLS clients and stores them in a secret named `skupper-service-client`.
--

. Modify the frontend deployment to include the generated certificates, for example:
+
[source,yaml]
----
spec:
  template:
    spec:
      containers:
      ...
        volumeMounts:
        - name: certs
          mountPath: /tmp/certs/skupper-service-client
      ...
      volumes:
      - name: certs
        secret:
          secretName: skupper-service-client

----

. Test calling the service from a TLS enabled frontend.

// Type: reference
[discrete#cli:index:::protocols,id='protocols']
=== Supported standards and protocols

Skupper supports the following protocols for your {service-network}:

* TCP - default
* HTTP1
* HTTP2

When exposing or creating a service, you can specify the protocol, for example:

[source,bash,options="nowrap"]
----
$ skupper expose deployment hello-world-backend --port 8080 --protocol <protocol>
----

where `<protocol>` can be:

* tcp
* http
* http2


When choosing which protocol to specify, note the following:

* `tcp` supports any protocol overlayed on TCP, for example, HTTP1 and HTTP2 work when you specify `tcp`.
* If you specify `http` or `http2`, the IP address reported by a client may not be accessible.
* All {service-network} traffic is converted to AMQP messages in order to traverse the {service-network}.
+
TCP is implemented as a single streamed message, whereas HTTP1 and HTTP2 are implemented as request/response message routing.

// Type: reference
[discrete#cli:index:::cli-global-options,id="cli-global-options"]
=== CLI options

For a full list of options, see the <<cli-reference:skupper:::,Kubernetes>> and <<cli-podman:skupper:::,Podman>> reference documentation.

[WARNING]
====
When you create a site and set logging level to `trace`, you can inadvertently log sensitive information from HTTP headers.

----
$ skupper init --router-logging trace
----


====

By default, all `skupper` commands apply to the cluster you are logged into and the current namespace.
The following `skupper` options allow you to override that behavior and apply to all commands:

`--namespace <namespace-name>`:: Apply command to `<namespace-name>`. For example, if you are currently working on `frontend` namespace and want to initialize a site in the `backend` namespace:
+
----
$ skupper init --namespace backend
----
`--kubeconfig <kubeconfig-path>`:: Path to the kubeconfig file - This allows you run multiple sessions to a cluster from the same client. An alternative is to set the `KUBECONFIG` environment variable.

`--context <context-name>`:: The kubeconfig file can contain defined contexts, and this option allows you to use those contexts.

:docname: skupper
:page-module: cli-reference
:page-relative-src-path: skupper.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#cli-reference:skupper:::]
== CLI reference

.Synopsis

.Options

```
-c, --context string      The kubeconfig context to use
-h, --help                help for skupper
--kubeconfig string   Path to the kubeconfig file to use
-n, --namespace string    The Kubernetes namespace to use
--platform string     The platform type to use [kubernetes, podman]
```

.See also

* https://skupper.io/skupper/latest/cli-reference/skupper_completion.html[skupper completion]	 - Output shell completion code for bash
* https://skupper.io/skupper/latest/cli-reference/skupper_debug.html[skupper debug]	 - Debug skupper installation
* https://skupper.io/skupper/latest/cli-reference/skupper_delete.html[skupper delete]	 - Delete skupper installation
* https://skupper.io/skupper/latest/cli-reference/skupper_expose.html[skupper expose]	 - Expose a set of pods through a Skupper address
* https://skupper.io/skupper/latest/cli-reference/skupper_gateway.html[skupper gateway]	 - Manage skupper gateway definitions
* https://skupper.io/skupper/latest/cli-reference/skupper_init.html[skupper init]	 - Initialise skupper installation
* https://skupper.io/skupper/latest/cli-reference/skupper_link.html[skupper link]	 - Manage skupper links definitions
* https://skupper.io/skupper/latest/cli-reference/skupper_network.html[skupper network]	 - Show information about the sites and services included in the network.
* https://skupper.io/skupper/latest/cli-reference/skupper_revoke-access.html[skupper revoke-access]	 - Revoke all previously granted access to the site.
* https://skupper.io/skupper/latest/cli-reference/skupper_service.html[skupper service]	 - Manage skupper service definitions
* https://skupper.io/skupper/latest/cli-reference/skupper_status.html[skupper status]	 - Report the status of the current Skupper site
* https://skupper.io/skupper/latest/cli-reference/skupper_token.html[skupper token]	 - Manage skupper tokens
* https://skupper.io/skupper/latest/cli-reference/skupper_unexpose.html[skupper unexpose]	 - Unexpose a set of pods previously exposed through a Skupper address
* https://skupper.io/skupper/latest/cli-reference/skupper_update.html[skupper update]	 - Update skupper installation version
* https://skupper.io/skupper/latest/cli-reference/skupper_version.html[skupper version]	 - Report the version of the Skupper CLI and services

[discrete]
// Auto generated by spf13/cobra on 12-Jun-2023

:docname: index
:page-module: declarative
:page-relative-src-path: index.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#declarative:index:::]
== YAML configuration reference
//Category: skupper-declarative
// Type: assembly
[id="skupper-declarative"]

Using YAML files to configure Skupper allows you to use source control to track and manage Skupper network changes.

// Type: procedure
[discrete#declarative:index:::installing-using-yaml,id="installing-using-yaml"]
=== Installing Skupper using YAML


Installing Skupper using YAML provides a declarative method to install Skupper.
You can store your YAML files in source control to track and manage Skupper network changes.

.Prerequisites

* Access to a Kubernetes cluster

.Procedure

. Log into your cluster.
If you are deploying Skupper to be available for all namespaces, verify you have `cluster-admin` privileges.

. Deploy the site controller:

* To install Skupper into the current namespace deploy the site controller using the following YAML:
+
[subs=attributes+]
----
kubectl apply -f deploy-watch-current-ns.yaml
----
where the contents of `deploy-watch-current-ns.yaml` is specified in the <<declarative:index:::watch-current-reference>> appendix.

* To install Skupper for all namespaces:
+
.. Create a namespace named `skupper-site-controller`.

.. Deploy the site controller using the following YAML:
+
[subs=attributes+]
----
kubectl apply -f deploy-watch-all-ns.yaml
----
where the contents of `deploy-watch-all-ns.yaml` is specified in the <<declarative:index:::watch-all-reference>> appendix.

. Verify the installation.
+
----
$ oc get pods
NAME                                       READY   STATUS    RESTARTS   AGE
skupper-site-controller-84694bdbb5-n8slb   1/1     Running   0          75s
----

// Type: procedure
[discrete#declarative:index:::creating-using-yaml,id="creating-using-yaml"]
=== Creating a Skupper site using YAML



Using YAML files to create Skupper sites allows you to use source control to track and manage Skupper network changes.

.Prerequisites

* Skupper is installed in the cluster or namespace you want to target.
* You are logged into the cluster.

.Procedure

. Create a YAML file to define the site, for example, `my-site.yaml`:
+
--
[source, bash]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: skupper-site
data:
  name: my-site
  console: "true"
  console-user: "admin"
  console-password: "changeme"
  flow-collector: "true"
----
The YAML creates a site with a console and you can create tokens from this site.

To create a site that has no ingress:

----
apiVersion: v1
kind: ConfigMap
metadata:
  name: skupper-site
data:
  name: my-site
  ingress: "false"
----

--
. Apply the YAML file to your cluster:
+
----
kubectl apply -f ~/my-site.yml
----

.Additional resources

See the <<declarative:index:::site-config-reference>> section for more reference.

// Type: procedure
[discrete#declarative:index:::linking-sites-using-yaml,id="linking-sites-using-yaml"]
=== Linking sites using YAML

While it is not possible to declaratively link sites, you can create a token using YAML.

.Prerequisites

* Skupper is installed on the clusters you want to link.
* You are logged into the cluster.


.Procedure

. Log into the cluster you want to link to and change context to the namespace where Skupper is installed.
This site must have `ingress` enabled.

. Create a YAML file named `token-request.yml` to request a token:
+
----
apiVersion: v1
kind: Secret
metadata:
  labels:
    skupper.io/type: connection-token-request
  name: secret-name
----

. Apply the YAML to the namespace to create a secret.
+
----
$ kubectl apply -f token-request.yml
----

. Create the token YAML from the secret.
+
----
$ kubectl get secret -o yaml secret-name | yq 'del(.metadata.namespace)' > ~/token.yaml
----


. Log into the cluster you want to link from and change context to the namespace where Skupper is installed.

. Apply the token YAML.
+
----
$ kubectl apply -f token.yml
----

. Verify the link, allowing some time for the process to complete.
+
----
$ skupper link status --wait 60
----

//Category: skupper-annotations
// Type: assembly
[discrete#declarative:index:::skupper-annotations,id="skupper-annotations"]
=== Configuring services using annotations

After creating and linking sites, you can use Kubernetes annotations to control which services are available on the {service-network}.


// Type: procedure
[discrete#declarative:index:::exposing-services-annotations,id="exposing-services-annotations"]
==== Exposing simple services on a {service-network} using annotations

This section provides an alternative to the `skupper expose` command, allowing you to annotate existing resources to expose simple services on the {service-network}.

.Prerequisites

* A site with a service you want to expose

.Procedure

. Log into the namespace in your cluster that is configured as a site.

. Create a deployment, some pods, or a service in one of your sites, for example:
+
[source, bash]
----
$ kubectl create deployment hello-world-backend --image quay.io/skupper/hello-world-backend
----
+
This step is not Skupper-specific, that is, this process is unchanged from standard processes for your cluster.

. Annotate the kubernetes resource to create a service that can communicate on the {service-network}, for example:
+
--
[source, bash]
----
$ kubectl annotate deployment backend "skupper.io/address=backend" "skupper.io/port=8080" "skupper.io/proxy=tcp"
----

The annotations include:

* `skupper.io/proxy` - the protocol you want to use, `tcp`, `http` or `http2`.
This is the only annotation that is required.
For example, if you annotate a simple deployment named `backend` with `skupper.io/proxy=tcp`, the service is exposed as `backend` and the `containerPort` value of the deployment is used as the port number.

* `skupper.io/address` - the name of the service on the {service-network}.

* `skupper.io/port` - one or more ports for the service on the {service-network}.


[NOTE]
====
When exposing services, rather than other resources like deployments, you can use the `skupper.io/target` annotation to avoid modifying the original service.
For example, if you want to expose the `backend` service:

[source, bash]
----
$ kubectl annotate service backend "skupper.io/address=van-backend" "skupper.io/port=8080" \
"skupper.io/proxy=tcp" "skupper.io/target=backend"
----

This allows you to delete and recreate the `backend` service without having to apply the annotation again.
====

--

. Check that you have exposed the service:
+
--

[source, bash]
----
$ skupper service status
Services exposed through Skupper:
╰─ backend (tcp port 8080)
   ╰─ Targets:
      ╰─ app=hello-world-backend name=hello-world-backend
----

NOTE: The related targets for services are only displayed when the target is available on the current cluster.
--

// Type: reference
[discrete#declarative:index:::understanding-annotations,id="understanding-annotations"]
==== Understanding Skupper annotations

Annotations allow you to expose services on the {service-network}.
This section provides details on the scope of those annotations


skupper.io/address::
The name of the service on the {service-network}.
Applies to:
* Deployments
* StatefulSets
* DaemonSets
* Services

skupper.io/port::
The port for the service on the {service-network}.
Applies to:
* Deployments
* StatefulSets
* DaemonSets

skupper.io/proxy::
The protocol you want to use, `tcp`, `http` or `http2`.
Applies to:
* Deployments
* StatefulSets
* DaemonSets
* Services

skupper.io/target::
The name of the target service you want to expose.
Applies to:
* Services

skupper.io/service-labels::
A comma separated list of label keys and values for the exposed service.
You can use this annotation to set up labels for monitoring exposed services.
Applies to:
* Deployments
* DaemonSets
* Services

// Uncomment when we have docs for headless

// skupper.io/headless::
// Flag that indicates Skupper to generate a headless service
// Applies to:
// * StatefulSets


// Type: reference
[id="site-config-reference"]
[appendix#declarative:index:::site-config-reference]
=== Site ConfigMap YAML reference


Using YAML files to configure Skupper requires that you understand all the fields so that you provision the site you require.

The following YAML defines a Skupper site:

----
apiVersion: v1
data:
  name: my-site //<.>
  console: "true" //<.>
  flow-collector: "true" //<.>
  console-authentication: internal //<.>
  console-user: "username" //<.>
  console-password: "password" //<.>
  cluster-local: "false" //<.>
  edge: "false" //<.>
  service-sync: "true" //<.>
  ingress: "true" //<.>
kind: ConfigMap
metadata:
  name: skupper-site
----

<.> Specifies the site name.

<.> Enables the skupper console, defaults to `false`.
NOTE: You must enable `console` and `flow-collector` for the console to function.

<.> Enables the flow collector, defaults to `false`.

<.> Specifies the skupper console authentication method. The options are `openshift`, `internal`, `unsecured`.

<.> Username for the `internal` authentication option.

<.> Password for the `internal` authentication option.

<.> Only accept connections from within the local cluster, defaults to `false`.

<.> Specifies whether an edge site is created, defaults to `false`.

<.> Specifies whether the services are synchronized across the {service-network}, defaults to `true`.

<.> Specifies whether the site supports ingress, for example, to create tokens usable from remote sites.

NOTE: All ingress types are supported using the same parameters as the `skupper` CLI.

[id="watch-current-reference"]
[appendix#declarative:index:::watch-current-reference]
=== YAML for watching current namespace

The following example deploys Skupper to watch the current namespace.

----
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: skupper-site-controller
  labels:
    application: skupper-site-controller
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    application: skupper-site-controller
  name: skupper-site-controller
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  - pods
  - pods/exec
  - services
  - secrets
  - serviceaccounts
  - events
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
  - patch
- apiGroups:
  - apps
  resources:
  - deployments
  - statefulsets
  - daemonsets
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - route.openshift.io
  resources:
  - routes
  verbs:
  - get
  - list
  - watch
  - create
  - delete
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - networkpolicies
  verbs:
  - get
  - list
  - watch
  - create
  - delete
- apiGroups:
  - projectcontour.io
  resources:
  - httpproxies
  verbs:
  - get
  - list
  - watch
  - create
  - delete
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - rolebindings
  - roles
  verbs:
  - get
  - list
  - watch
  - create
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    application: skupper-site-controller
  name: skupper-site-controller
subjects:
- kind: ServiceAccount
  name: skupper-site-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: skupper-site-controller
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: skupper-site-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      application: skupper-site-controller
  template:
    metadata:
      labels:
        application: skupper-site-controller
    spec:
      serviceAccountName: skupper-site-controller
      # Please ensure that you can use SeccompProfile and do not use
      # if your project must work on old Kubernetes
      # versions < 1.19 or on vendors versions which
      # do NOT support this field by default
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: site-controller
        image: quay.io/skupper/site-controller:master
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          allowPrivilegeEscalation: false
        env:
        - name: WATCH_NAMESPACE
          valueFrom:
             fieldRef:
               fieldPath: metadata.namespace
----

[id="watch-all-reference"]
[appendix#declarative:index:::watch-all-reference]
=== YAML for watching all namespaces

The following example deploys Skupper to watch all namespaces.

----
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: skupper-site-controller
  namespace: skupper-site-controller
  labels:
    application: skupper-site-controller
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    application: skupper-site-controller
  name: skupper-site-controller
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  - pods
  - pods/exec
  - services
  - secrets
  - serviceaccounts
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - apps
  resources:
  - deployments
  - statefulsets
  - daemonsets
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - route.openshift.io
  resources:
  - routes
  verbs:
  - get
  - list
  - watch
  - create
  - delete
- apiGroups:
  - networking.k8s.io
  resources:
  - ingresses
  - networkpolicies
  verbs:
  - get
  - list
  - watch
  - create
  - delete
- apiGroups:
  - projectcontour.io
  resources:
  - httpproxies
  verbs:
  - get
  - list
  - watch
  - create
  - delete
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - rolebindings
  - roles
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - update
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - clusterrolebindings
  verbs:
  - create
- apiGroups:
  - rbac.authorization.k8s.io
  resources:
  - clusterroles
  verbs:
  - bind
  resourceNames:
  - skupper-service-controller
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    application: skupper-site-controller
  name: skupper-site-controller
subjects:
- kind: ServiceAccount
  name: skupper-site-controller
  namespace: skupper-site-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: skupper-site-controller
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: skupper-site-controller
  namespace: skupper-site-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      application: skupper-site-controller
  template:
    metadata:
      labels:
        application: skupper-site-controller
    spec:
      serviceAccountName: skupper-site-controller
      # Please ensure that you can use SeccompProfile and do not use
      # if your project must work on old Kubernetes
      # versions < 1.19 or on vendors versions which
      # do NOT support this field by default
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: site-controller
        image: quay.io/skupper/site-controller:1.3.0
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          allowPrivilegeEscalation: false
----

:docname: index
:page-module: operator
:page-relative-src-path: index.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#operator:index:::]
== Using the Skupper operator

The {SkupperOperatorName} creates and manages Skupper sites in Kubernetes.

You can install the Operator as described in <<operator:index:::installing-operator-using-cli>>.


[NOTE]
====
Installing an Operator requires administrator-level privileges for your Kubernetes cluster.
====

After installing the Operator, you can create a site by deploying a ConfigMap as described in <<operator:index:::creating-site-using-operator>>


// Type: procedure
[discrete#operator:index:::installing-operator-using-cli,id='installing-operator-using-cli']
=== Installing the Operator using the CLI


The steps in this section show how to use the `kubectl` command-line interface (CLI) to install and deploy the latest version of the {SkupperOperatorName} in a given Kubernetes cluster.

.Prerequisites

* The Operator Lifecycle Manager is installed in the cluster.
For more information, see the link:https://olm.operatorframework.io/docs/getting-started/[QuickStart].

.Procedure

. Download the Skupper Operator example files, for example:
+
----
$ wget https://github.com/skupperproject/skupper-operator/archive/refs/heads/main.zip
----

. Create a `my-namespace` namespace.
NOTE: If you want to use a different namespace, you need to edit the referenced YAML files.

.. Create a new namespace:
+
[source,bash,options="nowrap",subs="+quotes"]
----
$ kubectl create namespace my-namespace
----

.. Switch context to the namespace:
+
[source,bash,options="nowrap",subs="+quotes"]
----
$ kubectl config set-context --current --namespace=my-namespace
----

. Create a CatalogSource in the `openshift-marketplace` namespace:
+
[source,bash,options="nowrap",subs=attributes+]
----
$ kubectl apply -f examples/k8s/00-cs.yaml
----

. Verify the skupper-operator catalog pod is running before continuing:
+
[source,bash,options="nowrap",subs=attributes+]
----
$ kubectl -n olm get pods | grep skupper-operator
----

. Create an OperatorGroup in the `my-namespace` namespace:
+
[source,bash,options="nowrap",subs=attributes+]
----
$ kubectl apply -f examples/k8s/10-og.yaml
----

. Create a Subscription  in the `my-namespace` namespace:
+
[source,bash,options="nowrap",subs=attributes+]
----
$ kubectl apply -f examples/k8s/20-sub.yaml
----

. Verify that the Operator is running:
+
[source,bash,options="nowrap"]
----
$ kubectl get pods -n my-namespace

NAME                                     READY   STATUS    RESTARTS   AGE
skupper-site-controller-d7b57964-gxms6   1/1     Running   0          1m
----
+
If the output does not report the pod is running, use the following command to determine the issue that prevented it from running:
+
----
$ kubectl describe pod -l name=skupper-operator
----

// Type: procedure
[discrete#operator:index:::creating-site-using-operator,id='creating-site-using-operator']
=== Creating a site using the Skupper Operator


. Create a YAML file defining the ConfigMap of the site you want to create.
+
--
For example, create `skupper-site.yaml` that provisions a site with a console:

[source,yaml,options="nowrap"]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: skupper-site
  namespace: my-namespace
data:
  console: "true"
  flow-collector: "true"
  console-user: "admin"
  console-password: "changeme"

----

NOTE: The console is a preview feature and may change before becoming fully supported by https://skupper.io[skupper.io].
Currently, you must enable the console on the same site as you enable the flow collector. This requirement may change before the console is fully supported by https://skupper.io[skupper.io].

You can also create a site without a console:

[source,yaml,options="nowrap"]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: skupper-site
  namespace: my-namespace
----
--

. Apply the YAML to create a ConfigMap named `skupper-site` in the namespace you want to use:
+
[source,bash,options="nowrap"]
----
$ kubectl apply -f skupper-site.yaml
----

. Verify that the site is created by checking that the Skupper router and service controller pods are running:
+
[source,bash,options="nowrap"]
----
$ kubectl get pods

NAME                                          READY   STATUS    RESTARTS   AGE
skupper-router-8c6cc6d76-27562                1/1     Running   0          40s
skupper-service-controller-57cdbb56c5-vc7s2   1/1     Running   0          34s
----
+
NOTE: If you deployed the Operator to a single namespace, an additional site controller pod is also running.

:docname: index
:page-module: kubernetes
:page-relative-src-path: index.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#kubernetes:index:::]
== Creating sites using a custom certificate authority
// Type: procedure
[id="custom-certs"]

By default:

* Network traffic between pods and the Skupper router is not encrypted. To encrypt traffic between pods and the Skupper router see <<kubernetes:service-certs:::>>.

* Skupper creates certificates to establish links between sites using mutual TLS.
This ensures that traffic between sites is encrypted.

These certificates are stored as secrets in the namespace when you create a site using `skupper init`.
If you want to use your own certificates, you can populate the a set of secrets with the appropriate certificates before creating the site as described in this section.
This set of secrets provides Skupper with the configuration required to create a site.

The following certificates are required:

skupper-claims-server:: Used for linking sites with claim type tokens.
skupper-console-certs:: Used by the Skupper console.
skupper-local-client and skupper-local-server:: Used by the Skupper router.
skupper-site-server:: Used for all inter-router connections, and for headless services.
skupper-service-client:: Used for services exposed over TLS.


.Prerequisites
* Access to a Kubernetes cluster with sufficient permission to run `skupper init`.
* Access to create certificates using your certificate authority.

.Procedure
. Create one or more certificates for a site.
+
--
There are several alternative approaches to this step:

* Reissue an existing certificate with a set of Subject Alternative Names (SANs) for the site.
* Create a new certificate with a set of SANs for the site.
* Create a new certificate for each item relating to the site.

You require a certificate for each of the following secrets:

* `skupper.<namespace>`
* `skupper-router.<namespace>`
* `skupper-router-local`
* `skupper-router-local.<namespace>.svc.cluster.local`
* `claims-<namespace>.<clustername>.<domain>`
* `skupper-<namespace>.<clustername>.<domain>`
* `skupper-edge-<namespace>.<clustername>.<domain>`
* `skupper-inter-router-<namespace>.<clustername>.<domain>`

where:

* `<namespace>` is the name of the namespace where you want to create a site.
* `<clustername>` is the name of the cluster.
* `<domain>` is the domain name for the cluster.

Using a specific certificate authority technology is beyond the scope of this guide. However, the following commands show how to create a certificate authority on Linux and create a single certificate that you can use to populate the secrets.

.. Create a `ca` directory and create a certificate authority certificate:
+
[source, bash]
----
$ mkdir ca

$ cd ca

$ ssh-keygen -t rsa -m PEM -f tls.key -q -N ""
$ openssl req -x509 -nodes -days 365 -key tls.key -out tls.crt
----

.. Given the certificate authority created `tls.crt` and `tls.key` files, you can create a certificate for the site as follows:
+
[source, bash]
----
$ cd ..
$ mkdir certificate
$ cd certificate

$ openssl req -nodes -newkey rsa:4096 -x509 -CA ../ca/tls.crt -CAkey ../ca/tls.key -out tls.crt -keyout tls.key -addext "subjectAltName = DNS:skupper.<namespace>, DNS:skupper-router.<namespace>, DNS:skupper-router-local, DNS:skupper-router-local.<namespace>.svc.cluster.local,DNS:claims-<namespace>.<clustername>.<domain>, DNS:skupper-<namespace>.<clustername>.<domain>, DNS:skupper-edge-<namespace>.<clustername>.<domain>, DNS:skupper-inter-router-<namespace>.<clustername>.<domain>"
----


You should now have a root certificate in the `ca` directory and another certificate in the `certificate` directory that you can use with a site.


--

. Create secrets for the site
+
--
.. Change to the parent directory of the `certificate` directory:
+
[source, bash]
----
$ cd ..
----

.. Populate the `ca` related secrets using the certificate from the `ca` directory:
+
[source, bash]
----
$ kubectl create secret tls skupper-site-ca --cert=ca/tls.crt --key=ca/tls.key

$ kubectl create secret tls skupper-service-ca --cert=ca/tls.crt --key=ca/tls.key

$ kubectl create secret tls skupper-local-ca --cert=ca/tls.crt --key=ca/tls.key

----

.. Populate the other secrets and modify them into the format required by `skupper`:
+
[source, bash]
----
$ kubectl create secret tls skupper-claims-server --cert=certificate/tls.crt --key=certificate/tls.key

$ kubectl patch secret skupper-claims-server  -p="{\"data\":{\"ca.crt\": \"$($ kubectl get secret skupper-site-ca -o json -o=jsonpath="{.data.tls\.crt}")\"}}"


$ kubectl create secret tls skupper-console-certs --cert=certificate/tls.crt --key=certificate/tls.key

$ kubectl patch secret skupper-console-certs  -p="{\"data\":{\"ca.crt\": \"$($ kubectl get secret skupper-local-ca -o json -o=jsonpath="{.data.tls\.crt}")\"}}"


$ kubectl create secret tls skupper-local-client --cert=certificate/tls.crt --key=certificate/tls.key

$ kubectl patch secret skupper-local-client  -p="{\"data\":{\"ca.crt\": \"$($ kubectl get secret skupper-local-ca -o json -o=jsonpath="{.data.tls\.crt}")\"}}"


$ kubectl create secret tls skupper-local-server --cert=certificate/tls.crt --key=certificate/tls.key

$ kubectl patch secret skupper-local-server  -p="{\"data\":{\"ca.crt\": \"$($ kubectl get secret skupper-local-ca -o json -o=jsonpath="{.data.tls\.crt}")\"}}"


$ kubectl create secret tls skupper-site-server --cert=certificate/tls.crt --key=certificate/tls.key

$ kubectl patch secret skupper-site-server  -p="{\"data\":{\"ca.crt\": \"$($ kubectl get secret skupper-site-ca -o json -o=jsonpath="{.data.tls\.crt}")\"}}"


$ kubectl create secret tls skupper-service-client --cert=certificate/tls.crt --key=certificate/tls.key

$ kubectl patch secret skupper-service-client  -p="{\"data\":{\"ca.crt\": \"$($ kubectl get secret skupper-service-ca -o json -o=jsonpath="{.data.tls\.crt}")\"}}"
----

--

. Create the site using the following command:
+
--
[source, bash]
----
$ skupper init
----

On OpenShift, `skupper` defaults to use the `route` ingress, which is the equivalent of `skupper init --ingress route`.

To verify your site, check the status:

[source, bash]
----
$ skupper status
----

[NOTE]
====
On OpenShift, you can also verify routes are created using:
[source, bash]
----
$ oc get routes
----
====
--

. Use the following command to check for errors relating to incorrect certificates:
+
[source, bash]
----
$ skupper debug events
----

:docname: service-certs
:page-module: kubernetes
:page-relative-src-path: service-certs.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#kubernetes:service-certs:::]
== Encrypting traffic from a pod to the Skupper router
// Type: assembly
[id="encrypting-traffic-pod-router"]

This section describes how to apply certificates to encrypt the traffic within a cluster.

By default:

* Skupper creates certificates to establish links between sites using mutual TLS so that traffic between sites is encrypted.
To use custom certificates for traffic between sites, see <<kubernetes:index:::>>.

* Network traffic between pods and the Skupper router is not encrypted.

You can use certificates to encrypt traffic between pods and the Skupper router as follows:

* <<kubernetes:service-certs:::skupper-generated-certs>> - does not require that you provide certificates.
* <<kubernetes:service-certs:::user-provided-certs>> - requires that the certificate authority, and the signed certificates, are distributed across all sites.

NOTE: With both procedures, you reference the certificates when exposing the service on the {service-network}.
If you do not reference certificates, the traffic between pods and the Skupper router is unencrypted.

// Type: procedure
[discrete#kubernetes:service-certs:::skupper-generated-certs,id="skupper-generated-certs"]
=== Exposing services on the {service-network} with Skupper-generated certificates

A Skupper installation includes a certificate authority which can generate certificates that can be used to encrypt traffic from the pod to the Skupper router.
This procedure describes how to use those certificates in your {service-network}.

.Prerequisites

* Access to the Kubernetes site where the service is exposed
* Access to the Kubernetes site where the service is called from
* service sync is enabled on both sites

.Procedure

. Expose the service on the {service-network}:

.. If you use the `expose` option:
+
--
[source, bash]
----
$ skupper expose  <target-type> <target-name> --generate-tls-secrets
----
For example, to expose a `backend` deployment using `http2`:
----
$ skupper expose deployment backend --port 8080 --protocol http2 --generate-tls-secrets
----
--

.. If you use `create` and `bind` options:
+
--
[source, bash]
----
$ skupper service create <service-name> --generate-tls-secrets
$ skupper service bind <service-name>  <target-type> <target-name>
----
--

. Check that the service is available on another site:
+
--
[source, bash]
----
$ skupper service status
Services exposed through Skupper:
╰─ nghttp2tls (http2 port 443)
----

On this site, a secret is created named `skupper-tls-<service-name>`.
The secret contains the generated certificates under `data/ca.crt`, `data/tls.crt`, and `data/tls.key`.
--

. Configure components that call the exposed service to use the certificates stored in `skupper-tls-<service-name>`.
+
--
For example, modify a deployment to mount the secret in a container.

----
      volumes:
      - name: certs
        secret:
          secretName: skupper-tls-nghttp2tls
----
--

. Configure the exposed service, which is the component that responds to the request, to use the certificates stored in `skupper-service-client`.
+
--
The `skupper-service-client` secret contains the certificate and private key of the Skupper certificate authority.
For example, modify a deployment to mount the secret in a container.

----
      volumes:
      - name: certs
        secret:
          secretName: skupper-service-client
----
--


// Type: procedure
[discrete#kubernetes:service-certs:::user-provided-certs,id="user-provided-certs"]
=== Exposing services on the {service-network} with user-provided certificates

You can encrypt traffic from the pod to the Skupper router using certificates provided by a certificate authority.

.Prerequisites

* Access to the Kubernetes site where the service is exposed
* Access to the Kubernetes site where the service is called from
* Certificate authority access (intermediate certificate is sufficient)


.Procedure

. Create a TLS secret from the certificate authority to store the private key and certificate.
+
--
The required format of the secret is:

`data/ca.crt`:: CA TLS certificate

For example, you might name the secret `ca-tls-secret`:

----
$ kubectl create secret generic ca-tls-secret --from-file=ca.crt=rootCA.crt
----
--

. Create a secret from the signed certificate and private key files:
+
--
The required format of the secret is:

`data/ca.crt`:: CA TLS certificate from step 1
`data/tls.crt`:: Signed TLS certificate
`data/tls.key`:: Signed Private key

For example, to encrypt a service named `backend`, you might name the secret `user-tls-backend`:
----
$ kubectl create secret tls user-tls-backend --key <key-path> --cert <cert-path>
$ kubectl patch secret user-tls-backend  -p="{\"data\":{\"ca.crt\": \"$(kubectl get secret ca-tls-secret -o json -o=jsonpath="{.data.tls\.crt}")\"}}"
----
--

. Expose the service on the {service-network}:

.. If you use the `expose` option, you specify the certificate secret and the CA secret, for example:
+
----
$ skupper expose deployment backend --port 5432 --protocol http2 --tls-cert user-tls-backend --tls-trust ca-tls-secret
----

.. If you use the `create` and `bind` options:
+
--
[source, bash]
----
$ skupper service create backend 5432 --tls-cert user-tls-backend
$ skupper bind deployment backend  --port 5001  --protocol http2 --tls-trust ca-tls-secret
----
--

NOTE: When certificates expire, you need to perform this procedure again with the new certificates.

:!sectids:
== <strong>Podman</strong>
:sectids:

:docname: podman
:page-module: cli
:page-relative-src-path: podman.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#cli:podman:::]
== CLI guide
:context: skupper-podman
//Category: skupper-cli
// Type: assembly
[id='using-skupper-podman']


Skupper podman allow you to create a site using containers, without requiring Kubernetes.
Typically, you create a site on a Linux host, allowing you to link to and from other sites, regardless of whether those sites are running in podman or Kubernetes.

NOTE: This is a preview feature and may change before becoming fully supported by https://skupper.io[skupper.io].


[discrete#cli:podman:::about,id='about']
=== About Skupper podman

Skupper podman is available with the following precedence:

`skupper --platform podman <command>`:: Use this option to avoid changing mode, for example, if you are working on Kubernetes and podman simultaneously.

`export SKUPPER_PLATFORM=podman`:: Use this command to use Skupper podman for the current session, for example, if you have two terminals set to different contexts. To set the environment to target Kubernetes sites:
+
----
$ export SKUPPER_PLATFORM=kubernetes
----

`skupper switch podman`:: If you enter this command, all subsequent command target podman rather than Kubernetes for all terminal sessions.


To determine which mode is currently active:

----
$ skupper switch

podman
----

To switch back to target Kubernetes sites: `skupper switch kubernetes`

[discrete#cli:podman:::creating-a-site,id='creating-a-site']
=== Creating a site using Skupper podman

.Prerequisites

* The latest `skupper` CLI is installed.
* Podman is installed, see https://podman.io/


. Set your session to use Skupper podman:
+
--
[source, bash]
----
$ export SKUPPER_PLATFORM=podman
----

To verify the `skupper` mode:

----
$ skupper switch

podman
----

--

. Create a Skupper site:
+
--
The simplest Skupper site allows you to link to other sites, but does not support linking _to_ the current site.

----
$ skupper init --ingress none

It is recommended to enable lingering for <username>, otherwise Skupper may not start on boot.
Skupper is now installed for user '<username>'.  Use 'skupper status' to get more information.
----

If you require that other sites can link to the site you are creating:

----
$ skupper init --ingress-host <machine-address>

It is recommended to enable lingering for <username>, otherwise Skupper may not start on boot.
Skupper is now installed for user '<username>'.  Use 'skupper status' to get more information.
----

For more information, see https://skupper.io/skupper/latest/cli-podman/skupper_init.html[podman skupper init].

--

. Check the status of your site:
+
--
[source, bash]
----
$ skupper status
Skupper is enabled for "<username>" with site name "<machine-name>-<username>" in interior mode. It is not connected to any other sites. It has no exposed services.
----

NOTE: You can only create one site per user. If you require a host to support many sites, create a user for each site.

--


[discrete#cli:podman:::linking-sites-using-skupper-podman]
=== Linking sites using Skupper podman

The general flow for linking podman sites is the same as for Kubernetes sites:

. Generate a token on one site:
+
----
$ skupper token create <filename>
----

. Create a link from the other site:
+
----
$ skupper link create <filename>
----

After you have linked to a network, you can check the link status:

----
$ skupper link status
----


[discrete#cli:podman:::working-with-services-using-skupper-podman]
=== Working with services using Skupper podman

The general flow for working with services is the same for Kubernetes and Podman sites.


NOTE: Services exposed on Kubernetes are not automatically available to Podman sites.
This is the equivalent to Kubernetes sites created using `skupper init --enable-service-sync false`.

.Example 01: Consuming a service from a Podman site

In this variation of the link:https://github.com/skupperproject/skupper-example-hello-world[hello world] example, the `backend` service is exposed on Kubernetes site and a Podman site is linked.
You deploy the `frontend` as a container and that container can access the `backend` service.

. Create a Podman site and link it to a Kubernetes site.

. Check the service from the Podman site:
+
----
$ skupper service status

No services defined
----
+
This result is expected because services exposed on Kubernetes are not automatically available to Podman sites.

. Create a service on the Podman site matching the service exposed on the Kubernetes site:
+
----
$ skupper service create backend 8080
----

. Validate the service from the Podman site by checking the backend API health URL:
+
--
[source, bash]
----
$ podman run -it --rm --network=skupper --name=myubi ubi8/ubi curl backend:8080/api/health

OK
----

This command runs a container using the `skupper` network and returns the results from `http://backend:8080/api/health`
--

. Run the frontend as a container:
+
----
$ podman run -dp 8080:8080 --name hello-world-frontend --network skupper quay.io/skupper/hello-world-frontend
----

. Check your {service-network} is working as expected by navigating to http://localhost:8080 and click *Say hello*.
+
--
Each of the backend replicas respond, for example `Hi, Perfect Parrot. I am Kind Hearted Component (backend-7c84887f9f-wxhxp).`

[NOTE]
====
In this scenario, running the `skupper service status` command on the Podman site does not provide much detail about the service:

----
$ skupper service status
Services exposed through Skupper:
╰─ backend (tcp port 8080)
----

====
--

.Example 02: Exposing a service from a Podman site

In this variation of the link:https://github.com/skupperproject/skupper-example-hello-world[hello world] example, the `backend` service is exposed on Podman site and consumed from a `frontend` on a Kubernetes site.



. Create a Podman site and link it to a Kubernetes site.

. Create and expose a frontend deployment on the Kubernetes site:
+
----
$ kubectl create deployment frontend --image quay.io/skupper/hello-world-frontend
$ kubectl expose deployment/frontend --port 8080 --type LoadBalancer
----

. Run the backend as a container:
+
----
$ podman run -d --name hello-world-backend --network skupper quay.io/skupper/hello-world-backend
----

. Expose the `backend` from the Podman site.
+
----
$ skupper expose host hello-world-backend --address backend --port 8080
----

. From the Kubernetes site, create the `backend` service:
+
----
$ skupper service create backend 8080
----


. Check your {service-network} is working as expected by navigating to your cluster URL, port 8080, and clicking *Say hello*.

For more information, see https://skupper.io/skupper/latest/cli-podman/skupper_expose.html[podman skupper expose].

:docname: skupper
:page-module: cli-podman
:page-relative-src-path: skupper.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#cli-podman:skupper:::]
== Podman Reference

.Synopsis

.Options

```
-h, --help              help for skupper
--platform string   The platform type to use [kubernetes, podman]
```

.See also

* https://skupper.io/skupper/latest/cli-podman/skupper_delete.html[skupper delete]	 - Delete skupper installation
* https://skupper.io/skupper/latest/cli-podman/skupper_expose.html[skupper expose]	 - Expose a set of pods through a Skupper address
* https://skupper.io/skupper/latest/cli-podman/skupper_init.html[skupper init]	 - Initialise skupper installation
* https://skupper.io/skupper/latest/cli-podman/skupper_link.html[skupper link]	 - Manage skupper links definitions
* https://skupper.io/skupper/latest/cli-podman/skupper_revoke-access.html[skupper revoke-access]	 - Revoke all previously granted access to the site.
* https://skupper.io/skupper/latest/cli-podman/skupper_service.html[skupper service]	 - Manage skupper service definitions
* https://skupper.io/skupper/latest/cli-podman/skupper_status.html[skupper status]	 - Report the status of the current Skupper site
* https://skupper.io/skupper/latest/cli-podman/skupper_token.html[skupper token]	 - Manage skupper tokens
* https://skupper.io/skupper/latest/cli-podman/skupper_unexpose.html[skupper unexpose]	 - Unexpose a set of pods previously exposed through a Skupper address
* https://skupper.io/skupper/latest/cli-podman/skupper_version.html[skupper version]	 - Report the version of the Skupper CLI and services

[discrete]
// Auto generated by spf13/cobra on 12-Jun-2023

:!sectids:
== <strong>Observability</strong>
:sectids:

:docname: index
:page-module: console
:page-relative-src-path: index.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#console:index:::]
== Console
[id="skupper-console"]



The Skupper console provides data and visualizations of the traffic flow between Skupper sites.

NOTE: This is a preview feature and may change before becoming fully supported by https://skupper.io[skupper.io].


// Type: procedure
[discrete#console:index:::enabling-console,id="enabling-console"]
=== Enabling the Skupper console

By default, when you create a Skupper site, a Skupper console is not available.

When enabled, the Skupper console URL is displayed whenever you check site status using `skupper status`.

.Prerequisites

* A Kubernetes namespace where you plan to create a site

.Procedure

. Determine which site in your {service-network} is best to enable the console.
+
--
Enabling the console also requires that you enable the flow-collector component, which requires resources to process traffic data from all sites.
You might locate the console using the following criteria:

* Does the {service-network} cross a firewall?
For example, if you want the console to be available only inside the firewall, you need to locate the flow-collector and console on a site inside the firewall.

* Is there a site that processes more traffic than other sites?
For example, if you have a _frontend_ component that calls a set of services from other sites, it might make sense to locate the flow collector and console on that site to minimize data traffic.

* Is there a site with more or cheaper resources that you want to use?
For example, if you have two sites, A and B, and resources are more expensive on site A, you might want to locate the flow collector and console on site B.
--

. Create a site with the flow collector and console enabled:
+
----
$ skupper init --enable-console --enable-flow-collector
----
+
IMPORTANT: Currently, you must enable the console on the same site as you enable the flow collector. This requirement may change before the console is fully supported by https://skupper.io[skupper.io].


// Type: procedure
[discrete#console:index:::accessing-console,id="accessing-console"]
=== Accessing the Skupper console

By default, the Skupper console is protected by credentials available in the `skupper-console-users` secret.


.Procedure

. Determine the Skupper console URL using the `skupper` CLI, for example:
+
----
$ skupper status

Skupper is enabled for namespace "west" in interior mode. It is not connected to any other sites. It has no exposed services.
The site console url is:  https://skupper-west.apps-crc.testing
----

. Browse to the Skupper console URL.
The credential prompt depends on how the site was created using `skupper init`:
+
* Using the `--console-auth unsecured` option, you are not prompted for credentials.
* Using the `--console-auth openshift` option, you are prompted to enter OpenShift cluster credentials.
* Using the default or `--console-user <user>  --console-password <password>` options, you are prompted to enter those credentials.

. If you created the site using default settings, that is `skupper init`, a random password is generated for the `admin` user.
To retrieve the password the `admin` user:
+
----
$ kubectl get secret skupper-console-users -o jsonpath={.data.admin} | base64 -d

JNZWzMHtyg
----

// Type: procedure
[discrete#console:index:::exploring-console,id="exploring-console"]
=== Exploring the Skupper console

After exposing a service on the {service-network}, you create an _address_, that is, a service name and port number associated with a site.
There might be many replicas associated with an address.
These replicas are shown in the Skupper console as _processes_.
Not all participants on a {service-network} are services.
For example, a _frontend_ deployment might call an exposed service named _backend_, but that frontend is not part of the {service-network}.
In the console, both are shown so that you can view the traffic and these are called _components_.

The Skupper console provides an overview of the following:

* Topology
* Addresses
* Sites
* Components
* Processes


The Skupper console also provides useful networking information about the {service-network}, for example, traffic levels.

image::skupper/latest/_images/skupper-adservice.png[]


. Check the *Sites* tab.
All your sites should be listed.
See the *Topology* tab to view how the sites are linked.

. Check that all the services you exposed are visible in the *Components* tab.

. Click a component to show the component details and associated processes.

. Click on a process to display the process traffic.
+
NOTE: The process detail displays the associated image, host, and addresses.
You can also view the clients that are calling the process.

. Click *Addresses* and choose an address to show the details for that address. This shows the set of servers that are exposed across the {service-network}.


TIP: To view information about each window, click the *?* icon.


// To view a static representation of all the console features, see the https://skupper-console-vry5.vercel.app/[Example Console].

:!sectids:
== <strong>Security</strong>
:sectids:

:docname: tokens
:page-module: cli
:page-relative-src-path: tokens.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#cli:tokens:::]
== Using Skupper tokens
//Category: skupper-tokens
// Type: assembly
[id='using-skupper-tokens']



Skupper tokens allow you to create links between sites.
You create a token on one site and use that token from the other site to create a link between the two sites.

NOTE: Although the linking process is directional, a Skupper link allows communication in both directions.

If both sites are equally accessible, for example, two public clouds, then it is not important where you create the token.
However, when using a token, the site you link to must be accessible from the site you link from.
For example, if you are creating a {service-network} using both a public and private cluster, you must create the token on the public cluster and use the token from the private cluster.

There are two types of Skupper token:

Claim token (default)::
+
--
A claim token can be restricted by:

* time - prevents token reuse after a specified period.
* usage - prevents creating multiple links from a single token.

All inter-site traffic is protected by mutual TLS using a private, dedicated certificate authority (CA).
A claim token is not a certificate, but is securely exchanged for a certificate during the linking process.
By implementing appropriate restrictions (for example, creating a single-use claim token), you can avoid the accidental exposure of certificates.
--

Cert token ::
+
--
You can use a cert token to create a link to the site which issued that token, it includes a valid certificate from that site.

All inter-site traffic is protected by mutual TLS using a private, dedicated certificate authority (CA).
A cert token is a certificate issued by the dedicated CA.
Protect it appropriately.
--

// Type: procedure
[discrete#cli:tokens:::creating-claim-tokens,id='creating-claim-tokens']
=== Creating claim tokens


You can use a claim token to create a link to the site which issued that token.
It does not includes a certificate from that site, but a certificate is passed from the site when the claim token is used.
A claim token can be restricted by time or usage.

.Procedure

. Log into the cluster.

. Change to the namespace associated with the site.

. Create a claim token, for example:
+
--
[source,bash,options="nowrap"]
----
$ skupper token create $HOME/secret.yaml --expiry 30m0s --uses 2 -t claim
----

NOTE: Claim tokens are the default, the `-t claim` section of the command is unnecessary.

--expiry:: The amount of time the token is valid in minutes and seconds, default `15m0s`.
--uses:: The number of times you can use the token to create a  link, default `1`.
--

.Additional information

* See the <<cli:index:::>> for information about using the token to create links.

// Type: procedure
[discrete#cli:tokens:::creating-cert-tokens,id='creating-cert-tokens']
=== Creating cert tokens


A cert token allows you create many links to the {service-network} from different sites without restrictions.

.Procedure

. Log into the cluster.

. Change to the namespace associated with the site.

. Create a cert token:
+
--
[source,bash,options="nowrap"]
----
$ skupper token create $HOME/secret.yaml -t cert
----

NOTE: Cert tokens are always valid and can be reused indefinitely unless revoked as described in <<cli:tokens:::revoking-access>>
--

.Additional information

* See the <<cli:index:::>> for information about using the token to create links.

// Type: procedure
[discrete#cli:tokens:::revoking-access,id='revoking-access']
=== Revoking access to a site


If a token is compromised, you can prevent unauthorized use of that token by invalidating  all the tokens created from a site.

This option removes all links to the site and requires that you recreate any links to restore the {service-network}.

. Procedure

. Log into the cluster.

. Change to the namespace associated with the site.

. Check the status of the site:
+
[source,bash,options="nowrap"]
----
$ skupper status
Skupper is enabled for namespace "west" in interior mode. It is linked to 2 other sites.
----

. Check outgoing links from the site:
+
[source,bash,options="nowrap"]
----
$ skupper link status
Link link1 is connected
----
+
In this case, there are two links, and one outgoing link, meaning there is one incoming link.

. Revoke access to the site from incoming links:
+
[source,bash,options="nowrap"]
----
$ skupper revoke-access
----

. Check the status of the site to see the revocation of access:
+
--
[source,bash,options="nowrap"]
----
$ skupper status
Skupper is enabled for namespace "west" in interior mode. It is linked to 1 other site.
$ skupper link status
Link link1 is connected
----

The output shows that the `skupper revoke-access` command has revoked the incoming links, but outgoing links are still connected.

You can remove that link using the `skupper link delete link1` command.
To revoke access, you must follow this procedure while logged into the appropriate cluster.

[NOTE]
====
After performing the `skupper revoke-access` command, the remote site still retains the link information and returns a `Already connected to <site>` message if you try to recreate the link.
To recreate the link, you must first delete the link manually from the remote site context.
====
--

:docname: native-security-options
:page-module: cli
:page-relative-src-path: native-security-options.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#cli:native-security-options:::]
== Securing a service network
// Type: assembly
[id="built-in-security-options"]

Skupper provides default, built-in security that scales across clusters and clouds.
This section describes additional security you can configure.

See <<policy:index:::>> for information about creating granular policies for each cluster.

// Type: procedure
[discrete#cli:native-security-options:::network-policy,id="network-policy"]
=== Restricting access to services using network-policy

By default, if you expose a service on the {service-network}, that service is also accessible from other namespaces in the cluster.
You can avoid this situation when creating a site using the `--create-network-policy` option.

.Procedure

. Create the {service-network} router with a network policy:
+
[source,bash]
----
$ skupper init --create-network-policy
----

. Check the site status:
+
--
[source,bash]
----
$ skupper status
----
The output should be similar to the following:
----
Skupper enabled for namespace 'west'. It is not connected to any other sites.
----
--

You can now expose services on the {service-network} and those services are not accessible from other namespaces in the cluster.


// Type: procedure
[discrete#cli:native-security-options:::tls,id="tls"]
=== Applying TLS to TCP or HTTP2 traffic on the {service-network}

By default, the traffic between sites is encrypted, however the traffic between the service pod and the router pod is not encrypted.
For services exposed as TCP or HTTP2, the traffic between the pod and the router pod can be encrypted using TLS.

.Prerequisites

* Two or more linked sites
* A TCP or HTTP2 frontend and backend service

.Procedure

. Deploy your backend service.

. Expose your backend deployment on the {service-network}, enabling TLS.
+
For example, if you want to expose a TCP service:
--
[source,bash]
----
$ skupper expose deployment <deployment-name> --port 443 --enable-tls
----

Enabling TLS creates the necessary certificates required for TLS backends and stores them in a secret named `skupper-tls-<deployment-name>`.
--

. Modify the backend deployment to include the generated certificates, for example:
+
--
[source,yaml]
----
...
    spec:
      containers:
      ...
        command:
        ...
        - "/certs/tls.key"
        - "/certs/tls.crt"
        ...
        volumeMounts:
        ...
        - mountPath: /certs
          name: certs
          readOnly: true
      volumes:
      - name: index-html
        configMap:
          name: index-html
      - name: certs
        secret:
          secretName: skupper-tls-<deployment-name>
----

Each site creates the necessary certificates required for TLS clients and stores them in a secret named `skupper-service-client`.
--

. Modify the frontend deployment to include the generated certificates, for example:
+
[source,yaml]
----
spec:
  template:
    spec:
      containers:
      ...
        volumeMounts:
        - name: certs
          mountPath: /tmp/certs/skupper-service-client
      ...
      volumes:
      - name: certs
        secret:
          secretName: skupper-service-client

----

. Test calling the service from a TLS enabled frontend.

:docname: index
:page-module: policy
:page-relative-src-path: index.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#policy:index:::]
== Securing a service network using policies
//Category: skupper-policy
// Type: assembly
[id="skupper-policy"]


By default, Skupper includes many security features, including using mutual TLS for all {service-network} communication between sites.
By default, applying the policy system to a cluster prevents all {service-network} communication to and from that cluster.
You specify granular policies to allow only the {service-network} communication you require.

NOTE: The policy system is distinct from the `network-policy` option which restricts access to Skupper services to the current namespace as described in <<cli:index:::>>.

Each site in a {service-network} runs a Skupper router and has a private, dedicated certificate authority (CA).
Communication between sites is secured with mutual TLS, so the {service-network} is isolated from external access, preventing security risks such as lateral attacks, malware infestations, and data exfiltration.
The policy system adds another layer at a cluster level to help a cluster administrator control access to a {service-network}.

This guide assumes that you understand the following Skupper concepts:

site:: A namespace in which Skupper is installed.
token:: A token is required to establish a link between two sites.
{service-network}:: After exposing services using Skupper, you have created a {service-network}.

// Type: concept
[discrete#policy:index:::about-skupper-policies,id="about-skupper-policies"]
=== About the policy system

After a cluster administrator installs the policy system using a Custom Resource Definition (CRD), the cluster administrator needs to configure one or more policies to allow _developers_ create and use services on the {service-network}.

NOTE: In this guide, _developers_ refers to users of a cluster who have access to a namespace, but do not have administrator privileges.

A cluster administrator configures one or more of following items using custom resources (CRs) to enable communication:

Allow incoming links:: Use `allowIncomingLinks` to enable developers create tokens and configure incoming links.

Allow outgoing links to specific hosts:: Use `allowedOutgoingLinksHostnames` to specify hosts that developers can create links to.

Allow services:: Use `allowedServices` to specify which services developers can create or use on the {service-network}.

Allow resources to be exposed:: Use `allowedExposedResources` to specify which resources a developer can expose on the {service-network}.

NOTE: A cluster administrator can apply each policy CR setting to one or more namespaces.

For example, the following policy CR fully allows all Skupper capabilities on all namespaces, except for:

* only allows outgoing links to any domain ending in `.example.com`.
* only allows 'deployment/nginx' resources to be exposed on the {service-network}.

[source,yaml]
----
apiVersion: skupper.io/v1alpha1
kind: SkupperClusterPolicy
metadata:
  name: cluster-policy-sample-01
spec:
  namespaces:
    - "*"
  allowIncomingLinks: true
  allowedExposedResources:
    - "deployment/nginx"
  allowedOutgoingLinksHostnames: [".*\\.example.com$"]
  allowedServices:
    - "*"
----

[NOTE]
====
You can apply many policy CRs, and if there are conflicts in the items allowed, the most permissive policy is applied.
For example, if you apply an additional policy CR with the line `allowedOutgoingLinksHostnames: []`, which does not list any hostnames, outgoing links to `*.example.com` are still permitted because that is permitted in the original CR.
====

`namespaces`:: One or more patterns to specify the namespaces that this policy applies to.
Note that you can use link:https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/[Label selectors] to match the namespaces.

`allowIncomingLinks`:: Specify `true` to allow other sites create links to the specified namespaces.

`allowedOutgoingLinksHostnames`:: Specify one or more patterns to determine which hosts you can create links to from the specified namespaces.

`allowedServices`:: Specify one or more patterns to determine the permitted names of services allowed on the {service-network} from the specified namespaces.

`allowedExposedResources`:: Specify one or more permitted names of resources allowed on the {service-network} from the specified namespaces.
Note that patterns are not supported.

[TIP]
====
Use regular expressions to create pattern matches, for example:

* `.*\\.com$` matches any string ending in `.com`.
A double backslash is required to avoid issues in YAML.
* `^abc$` matches the string `abc`.

====

If you create another CR that allows outgoing links for a specific namespace, a user can create a link from that namespace to join a {service-network}. That is, the logic for multiple policy CRs is `OR`.
An operation is permitted if any single policy CR permits the operation.

// Type: procedure
[discrete#policy:index:::installing-crd,id="installing-crd"]
=== Installing the policy system CRD

Installing the policy system CRD enables a cluster administrator to enforce policies for {service-network}s.

NOTE: If there are existing sites on the cluster, see <<policy:index:::installing-crd-existing-sites>> to avoid {service-network} disruption.

.Prerequisites

* Access to a cluster using a `cluster-admin` account
* The Skupper operator is installed

.Procedure

. Log in to the cluster using a `cluster-admin` account.

. Download the CRD:
+
[source,bash]
----
$ wget https://raw.githubusercontent.com/skupperproject/skupper/1.4/api/types/crds/skupper_cluster_policy_crd.yaml
----

. Apply the CRD:
+
[source,bash]
----
$ kubectl apply -f skupper_cluster_policy_crd.yaml

customresourcedefinition.apiextensions.k8s.io/skupperclusterpolicies.skupper.io created
clusterrole.rbac.authorization.k8s.io/skupper-service-controller created
----


. To verify that the policy system is active, use the `skupper status` command and check that the output includes the following line:
+
[source,bash]
----
Skupper is enabled for namespace "<namespace>" in interior mode (with policies).
----



// Type: procedure
[discrete#policy:index:::installing-crd-existing-sites,id="installing-crd-existing-sites"]
=== Installing the policy system CRD on a cluster with existing sites

If the cluster already hosts Skupper sites, note the following before installing the CRD:

* All existing connections are closed.
You must apply a policy CR to reopen connections.
* All existing {service-network} services and exposed resources are removed.
You must create those resources again.


.Procedure

To avoid disruption:

. Plan the CRD deployment for an appropriate time.

. Search your cluster for sites:
+
[source,bash]
----
$ kubectl get pods --all-namespaces --selector=app=skupper
----

. Document each service and resource exposed on the {service-network}.

. Install the CRD as described in <<policy:index:::installing-crd>>.
This step closes connections and removes all {service-network} services and exposed resources.

. If Skupper sites exist in the cluster not created by `cluster-admin`, you must grant permissions to read policies to developers to avoid that site being blocked from the {service-network}.
+
--
For each site namespace:

[source,bash]
----
$ kubectl create clusterrolebinding skupper-service-controller-<namespace> --clusterrole=skupper-service-controller --serviceaccount=<namespace>:skupper-service-controller
----

where `<namespace>` is the site namespace.
--


. Create policy CRs as described in <<policy:index:::creating-policies>>

. Recreate any services and exposed resources as required.


// Type: procedure
[discrete#policy:index:::creating-policies,id="creating-policies"]
=== Creating policies for the policy system

Policies allow a cluster administrator to control communication across the {service-network} from a cluster.


.Prerequisites

* Access to a cluster using a `cluster-admin` account.
* The policy system CRD is installed on the cluster.

.Procedure

NOTE: Typically, you create a policy CR that combines many elements from the steps below. See <<policy:index:::about-skupper-policies>> for an example CR.

. <<policy:index:::allowIncomingLinks>>
. <<policy:index:::allowedOutgoingLinksHostnames>>
. <<policy:index:::allowedServices>>
. <<policy:index:::allowedExposedResources>>

// Type: procedure
[discrete#policy:index:::allowIncomingLinks,id="allowIncomingLinks"]
==== Implement a policy to allow incoming links

Use `allowIncomingLinks` to enable developers create tokens and configure incoming links.

.Procedure

. Determine which namespaces you want to apply this policy to.
. Create a CR with `allowIncomingLinks` set to `true` or `false`.
. Create and apply the CR.

For example, the following CR allows incoming links for all namespaces:
[source,yaml]
----
apiVersion: skupper.io/v1alpha1
kind: SkupperClusterPolicy
metadata:
  name: allowincominglinks
spec:
  namespaces:
    - "*"
  allowIncomingLinks: true
----




// Type: procedure
[discrete#policy:index:::allowedOutgoingLinksHostnames,id="allowedOutgoingLinksHostnames"]
==== Implement a policy to allow outgoing links to specific hosts

Use `allowedOutgoingLinksHostnames` to specify hosts that developers can create links to.
You cannot create a `allowedOutgoingLinksHostnames` policy to disallow a specific host that was previously allowed.

. Determine which namespaces you want to apply this policy to.
. Create a CR with `allowedOutgoingLinksHostnames` set to a pattern of allowed hosts.
. Create and apply the CR.

For example, the following CR allows links to all subdomains of `example.com` for all namespaces:
[source,yaml]
----
apiVersion: skupper.io/v1alpha1
kind: SkupperClusterPolicy
metadata:
  name: allowedoutgoinglinkshostnames
spec:
  namespaces:
    - "*"
  allowedOutgoingLinksHostnames: ['.*\\.example\\.com']
----


// Type: procedure
[discrete#policy:index:::allowedServices,id="allowedServices"]
==== Implement a policy to allow specific services

Use `allowedServices` to specify which services a developer can create or use on the {service-network}.
You cannot create a `allowedServices` policy to disallow a specific service that was previously allowed.

.Procedure

. Determine which namespaces you want to apply this policy to.
. Create a CR with `allowedServices` set to specify the services allowed on the {service-network}.
. Create and apply the CR.

For example, the following CR allows users to expose and consume services with the prefix `backend-` for all namespaces:
[source,yaml]
----
apiVersion: skupper.io/v1alpha1
kind: SkupperClusterPolicy
metadata:
  name: allowedservices
spec:
  namespaces:
    - "*"
  allowedServices: ['^backend-']
----

NOTE: When exposing services, you can use the `--address <name>` parameter of the `skupper` CLI to name services to match your policy.


// Type: procedure
[discrete#policy:index:::allowedExposedResources,id="allowedExposedResources"]
==== Implement a policy to allow specific resources

Use `allowedExposedResources` to specify which resources a developer can expose on the {service-network}.
You cannot create a `allowedExposedResources` policy to disallow a specific resource that was previously allowed.

.Procedure

. Determine which namespaces you want to apply this policy to.
. Create a CR with `allowedExposedResources` set to specify resources that a developer can expose on the {service-network}.
. Create and apply the CR.

For example, the following CR allows you to expose an `nginx` deployment for all namespaces:
[source,yaml]
----
apiVersion: skupper.io/v1alpha1
kind: SkupperClusterPolicy
metadata:
  name: allowedexposedresources
spec:
  namespaces:
    - "*"
  allowedExposedResources: ['deployment/nginx']
----

NOTE: For `allowedExposedResources`, each entry must conform to the `type/name` syntax.

// Type: procedure
[discrete#policy:index:::exploring-policies,id="exploring-policies"]
=== Exploring the current policies for a cluster

As a developer you might want to check which policies are enforced for a particular site.

.Procedure

. Log into a namespace where a Skupper site has been initialized.

. Check whether incoming links are permitted:
+
[source,bash]
----
$ kubectl exec deploy/skupper-service-controller -- get policies incominglink

ALLOWED POLICY ENABLED ERROR                                                   ALLOWED BY
false   true           Policy validation error: incoming links are not allowed
----
+
In this example incoming links are not allowed by policy.

. Explore other policies:
+
--
[source,bash]
----
$ kubectl exec deploy/skupper-service-controller -- get policies
Validates existing policies

Usage:
  get policies [command]

Available Commands:
  expose       Validates if the given resource can be exposed
  incominglink Validates if incoming links can be created
  outgoinglink Validates if an outgoing link to the given hostname is allowed
  service      Validates if service can be created or imported
----

As shown, there are commands to check each policy type by specifying what you want to do, for example, to check if you can expose an nginx deployment:

[source,bash]
----
$ kubectl  exec deploy/skupper-service-controller -- get policies expose deployment nginx
ALLOWED POLICY ENABLED ERROR                                                       ALLOWED BY
false   true           Policy validation error: deployment/nginx cannot be exposed
----

If you allowed an nginx deployment as described in <<policy:index:::allowedExposedResources>>, the same command shows that the resource is allowed and displays the name of the policy CR that enabled it:

[source,bash]
----
$ kubectl  exec deploy/skupper-service-controller -- get policies expose deployment nginx
ALLOWED POLICY ENABLED ERROR                                                       ALLOWED BY
true    true                                                                       allowedexposedresources
----


--

:!sectids:
== <strong>Troubleshooting</strong>
:sectids:

:docname: index
:page-module: troubleshooting
:page-relative-src-path: index.adoc
:page-origin-url: https://github.com/pwright/skupper-docs.git
:page-origin-start-path:
:page-origin-refname: fixrootlink
:page-origin-reftype: branch
:page-origin-refhash: (worktree)
[#troubleshooting:index:::]
== Troubleshooting a service network
//Category: skupper-troubleshooting
// Type: assembly

[id="troubleshooting"]


Typically, you can create a {service-network} without referencing this troubleshooting guide.
However, this guide provides some tips for situations when the {service-network} does not perform as expected.

See <<troubleshooting:index:::common-problems>> if you have encountered a specific issue using the `skupper` CLI.

A typical troubleshooting workflow is to check all the sites and create debug tar files.

// Type: procedure
[discrete#troubleshooting:index:::checking-sites,id="checking-sites"]
=== Checking sites


Using the `skupper` command-line interface (CLI) provides a simple method to get started with troubleshooting Skupper.

.Procedure

. Check the site status:
+
--
[source, bash]
----
$ skupper status --namespace west

Skupper is enabled for namespace "west" in interior mode. It is connected to 2 other sites. It has 1 exposed services.
----

The output shows:

* A site exists in the specified namespace.
* A link exists to two other sites.
* A service is exposed on the {service-network} and is accessible from this namespace.

--

. Check the {service-network}:
+
--
[source, bash]
----
$ skupper network status --namespace west

Sites:
├─ [local] 05f8c38 - west
│  URL: 10.110.15.54
│  mode: interior
│  name: west
│  namespace: west
│  version: 1.0.2
│  ╰─ Services:
│     ╰─ name: backend
│        address: backend: 8080
│        protocol: tcp
╰─ [remote] 1537b82 - east
   URL: 10.97.26.100
   name: east
   namespace: east
   sites linked to: 05f8c38-west
   version: 1.0.2
   ╰─ Services:
      ╰─ name: backend
         address: backend: 8080
         protocol: tcp
         ╰─ Targets:
            ├─ name: backend-77f8f45fc8-smckp
            ├─ name: backend-77f8f45fc8-gh6tp
            ╰─ name: backend-77f8f45fc8-m58tg
----

NOTE: If the output is not what you expected, you might want to <<troubleshooting:index:::checking-links,check links>> before proceeding.

The output shows:

* There are 2 sites on the {service-network}, `east` and `west`.
* Details for each site, for example the namespace names.
* The original services that are exposed (Targets), in this case the three backend services exposed using the `tcp` protocol.
* The services available on the {service-network}, including the port number. For example, `backend:8080`.
--

. Check the status of services exposed on the {service-network}:
+
--
[source, bash]
----
$ skupper service status
Services exposed through Skupper:
╰─ backend (tcp port 8080)
   ╰─ Targets:
      ╰─ app=backend name=backend
----

The output shows the `backend` service and the related target of that service.

NOTE: The related targets for services are only displayed when the target is available on the current cluster.
--

. List the events for a site:
+
--
[source, bash]
----
$ skupper debug events
NAME                         COUNT                                                          AGE
GatewayQueryRequest          3                                                              9m12s
                             3     gateway request                                          9m12s
SiteQueryRequest             3                                                              9m12s
                             3     site data request                                        9m12s
ServiceControllerEvent       9                                                              10m24s
                             2     service event for west/frontend                          10m24s
                             1     service event for west/backend                           10m26s
                             1     Checking service for: backend                            10m26s
                             2     Service definitions have changed                         10m26s
                             1     service event for west/skupper-router                    11m4s
DefinitionMonitorEvent       15                                                             10m24s
                             2     service event for west/frontend                          10m24s
                             1     service event for west/backend                           10m26s
                             1     Service definitions have changed                         10m26s
                             5     deployment event for west/frontend                       10m34s
                             1     deployment event for west/skupper-service-controller     11m4s
ServiceControllerUpdateEvent 1                                                              10m26s
                             1     Updating skupper-internal                                10m26s
ServiceSyncEvent             3                                                              10m26s
                             1     Service interface(s) added backend                       10m26s
                             1     Service sync sender connection to                        11m4s
                                   amqps://skupper-router-local.west.svc.cluster.local:5671
                                   established
                             1     Service sync receiver connection to                      11m4s
                                   amqps://skupper-router-local.west.svc.cluster.local:5671
                                   established
IpMappingEvent               5                                                              10m34s
                             1     172.17.0.7 mapped to frontend-6b4688bf56-rp9hc           10m34s
                             2      mapped to frontend-6b4688bf56-rp9hc                     10m54s
                             1     172.17.0.4 mapped to                                     11m4s
                                   skupper-service-controller-6c97c5cf5d-6nzph
                             1     172.17.0.3 mapped to skupper-router-547dffdcbf-l8pdc     11m4s
TokenClaimVerification       1                                                              10m59s
                             1     Claim for efe3a241-3e4f-11ed-95d0-482ae336eb38 succeeded 10m59s

----
The output shows sites being linked and a service being exposed on a {service-network}.
However, this output is most useful when reporting an issue and is included in the Skupper debug tar file.
--

.Additional information

* <<troubleshooting:index:::checking-links>>

// Type: procedure
[discrete#troubleshooting:index:::checking-links,id='checking-links']
=== Checking links

You must link sites before you can expose services on the {service-network}.

NOTE: By default, tokens expire after 5 minutes and you can only use a token once.
Generate a new token if the link is not connected.
You can also generate tokens using the `-token-type cert` option for permanent reusable tokens.

This section outlines some advanced options for checking links.


. Check the link status:
+
--
[source, bash]
----
$ skupper link status --namespace east

Links created from this site:
-------------------------------
Link link1 is connected
----

A link exists from the specified site to another site, meaning a token from another site was applied to the specified site.

NOTE: Running `skupper link status` on a connected site produces output only if a token was used to create a link.

If you use this command on a site where you did not create the link, but there is an incoming link to the site:
----
$ skupper link status --namespace west

Links created from this site:
-------------------------------
There are no links configured or connected

Currently connected links from other sites:
----------------------------------------
A link from the namespace east on site east(536695a9-26dc-4448-b207-519f56e99b71) is connected
----
--

. Check the verbose link status:
+
--
[source, bash]
----
$ skupper link status link1 --verbose --namespace east

 Cost:          1
 Created:       2022-10-24 12:50:33 +0100 IST
 Name:          link1
 Namespace:     east
 Site:          east-536695a9-26dc-4448-b207-519f56e99b71
 Status:        Connected
----

The output shows detail about the link, including a timestamp of when the link was created and the associated relative cost of using the link.

The status of the link must be `Connected` to allow service traffic.
--

.Additional information

* <<troubleshooting:index:::checking-sites>>

// Type: procedure
[discrete#troubleshooting:index:::debug-gateways,id='debug-gateways']
=== Checking gateways

By default, `skupper gateway` creates a service type gateway and these gateways run properly after a machine restart.

However, if you create a docker or podman type gateway, check that the container is running after a machine restart.
For example:

. Check the status of Skupper gateways:
+
--
[subs=attributes+]
----
$ skupper gateway status

Gateway Definition:
╰─ machine-user type:podman version:{service-version}
   ╰─ Bindings:
      ╰─ mydb:3306 tcp mydb:3306 localhost 3306

----
This shows a podman type gateway.
--

. Check that the container is running:
+
--
[subs=attributes+]
----
$ podman ps
CONTAINER ID  IMAGE                                           COMMAND               CREATED         STATUS             PORTS                   NAMES
4e308ef8ee58  quay.io/skupper/skupper-router:{service-version}             /home/skrouterd/b...  26 seconds ago  Up 27 seconds ago                          machine-user

----
This shows the container running.

NOTE: To view stopped containers, use `podman ps -a` or `docker ps -a`.
--


. Start the container if necessary:
+
--
[subs=attributes+]
----

$ podman start machine-user

----
--




// Type: procedure
[discrete#troubleshooting:index:::creating-debug,id='creating-debug']
=== Creating a Skupper debug tar file

The debug tar file contains all the logs from the Skupper components for a site and provides detailed information to help debug issues.

. Create the debug tar file:
+
----
$  skupper debug dump my-site

Skupper dump details written to compressed archive:  `my-site.tar.gz`
----

. You can expand the file using the following command:
+
--
[source, bash]
----
$ tar -xvf kind-site.tar.gz

k8s-versions.txt
skupper-versions.txt
skupper-router-deployment.yaml
skupper-router-867f5ddcd8-plrcg-skstat-g.txt
skupper-router-867f5ddcd8-plrcg-skstat-c.txt
skupper-router-867f5ddcd8-plrcg-skstat-l.txt
skupper-router-867f5ddcd8-plrcg-skstat-n.txt
skupper-router-867f5ddcd8-plrcg-skstat-e.txt
skupper-router-867f5ddcd8-plrcg-skstat-a.txt
skupper-router-867f5ddcd8-plrcg-skstat-m.txt
skupper-router-867f5ddcd8-plrcg-skstat-p.txt
skupper-router-867f5ddcd8-plrcg-router-logs.txt
skupper-router-867f5ddcd8-plrcg-config-sync-logs.txt
skupper-service-controller-deployment.yaml
skupper-service-controller-7485756984-gvrf6-events.txt
skupper-service-controller-7485756984-gvrf6-service-controller-logs.txt
skupper-site-configmap.yaml
skupper-services-configmap.yaml
skupper-internal-configmap.yaml
skupper-sasl-config-configmap.yaml
----

These files can be used to provide support for Skupper, however some items you can check:

versions:: See `*versions.txt` for the versions of various components.

ingress:: See `skupper-site-configmap.yaml` to determine the `ingress` type for the site.

linking and services:: See the `skupper-service-controller-*-events.txt` file to view details of token usage and service exposure.

--

// Type:procedure
[discrete#troubleshooting:index:::router-performance,id='router-performance']
=== Improving Skupper router performance

If you encounter Skupper router performance issues, you can scale the Skupper router to address those concerns.

NOTE: Currently, you must delete and recreate a site to reconfigure the Skupper router.

For example, use this procedure to increase throughput, and if you have many clients, latency.

. Delete your site or create a new site in a different namespace.
+
Note all configuration and delete your existing site:
+
----
$ skupper delete
----
+
As an alternative, you can create a new namespace and configure a new site with optimized Skupper router performance.
After validating the performance improvement, you can delete and recreate your original site.

. Create a site with optimal performance CPU settings:
+
----
$ skupper init --router-cpu 5
----

. Recreate your configuration from step 1, recreating links and services.


NOTE: While you can address availability concerns by scaling the number of routers and somewhat improve performance, for example `skupper init --routers 3`, typically this is not necessary.




// Type:procedure
[discrete#troubleshooting:index:::common-problems,id='common-problems']
=== Resolving common problems

The following issues and workarounds might help you debug simple scenarios when evaluating Skupper.

*Cannot initialize skupper*

If the `skupper init` command fails, consider the following options:

* Check the load balancer.
+
--
If you are evaluating Skupper on minikube, use the following command to create a load balancer:

----
$ minikube tunnel
----

For other Kubernetes flavors, see the documentation from your provider.

--

* Initialize without ingress.
+
--
This option prevents other sites from linking to this site, but linking outwards is supported.
Once a link is established, traffic can flow in either direction.
Enter the following command:

[source,bash]
----
$ skupper init --ingress none
----

[NOTE]
====
See https://skupper.io/skupper/latest/cli-reference/skupper_init.html[] for more options.
====

--

*Cannot link sites*

To link two sites, one site must be accessible from the other site.
For example, if one site is behind a firewall and the other site is on an AWS cluster, you must:

. Create a token on the AWS cluster site.

. Create the link on the site inside the firewall.

[NOTE]
====
By default, a token is valid for only 15 minutes and can only be used once.
See <<cli:tokens:::>> for more information on creating different types of tokens.
====

*Cannot access Skupper console*

Starting with Skupper release 1.3, the console is not enabled by default.
To use the new console, which is a preview feature and may change, see <<console:index:::>>.

Use `skupper status` to find the console URL.

Use the following command to display the password for the `admin` user:doctype: article

----
$ kubectl get secret/skupper-console-users -o jsonpath={.data.admin} | base64 -d
----

*Cannot create a token for linking clusters*

There are several reasons why you might have difficulty creating tokens:

Site not ready::
+
--
After creating a site, you might see the following message when creating a token:
----
Error: Failed to create token: Policy validation error: Skupper is not enabled in namespace
----

Use `skupper status` to verify the site is working and try to create the token again.
--

No ingress::
+
--
You might see the following note after using the `skupper token create` command:
----
Token written to <path> (Note: token will only be valid for local cluster)
----

This output indicates that the site was deployed without an ingress option. For example `skupper init --ingress none`.
You must specify an ingress to allow sites on other clusters to link to your site.

You can also use the `skupper token create` command to check if an ingress was specified when the site was created.
--
